\documentclass[a4paper]{article}
\usepackage{ismir,amsmath,cite}

% The style file neglects to change math to Times.
\IfFileExists{newtxmath.sty}
{
  \usepackage{newtxmath}
  \newcommand*{\mathdefault}{ntxrx}
  \DeclareSymbolFont{operators}{\encodingdefault}{\mathdefault}{m}{n}
  \SetSymbolFont{operators}{bold}{\encodingdefault}{\mathdefault}{b}{n}
  \DeclareMathAlphabet{\mathit}{\encodingdefault}{\mathdefault}{m}{it}
  \DeclareMathAlphabet{\mathbf}{\encodingdefault}{\mathdefault}{b}{n}
  \SetMathAlphabet{\mathit}{bold}{\encodingdefault}{\mathdefault}{b}{it}
  % newtxmath imports the plus sign incorrectly
  \DeclareMathSymbol{+}{\mathbin}{operators}{`+}
}{
  \usepackage{txfonts}
}

% Use pro Times font for small caps if possible.
\IfFileExists{TimesLTStd.sty}
{
  \usepackage{TimesLTStd}
  \newcommand*{\acro}[1]{\textsc{\MakeLowercase{##1}}}
  % But small caps look off in headings.
  \usepackage{titlesec}
  \titleformat{\section}%
  {\fontfamily{TimesLTStd-TLF}\bfseries\selectfont\centering\uppercase}%
  {\thesection.}{0.6em}{}
  \titleformat{\subsection}%
  {\fontfamily{TimesLTStd-TLF}\bfseries\selectfont\raggedright}%
  {\thesubsection}{0.6em}{}
}{
  \newcommand*{\acro}[1]{\relax ##1}
}

% Reset fonts from the standard style file and make the formatting nice.
\renewcommand{\sfdefault}{phv}
\renewcommand{\ttdefault}{pcr}
\usepackage[tracking,letterspace=55,babel]{microtype}
\usepackage{url}

% Ensure that the input is internationally capable.
\usepackage[utf8]{inputenc}
\usepackage[british]{babel}
\usepackage{csquotes}

% Prepare nicer tables and figures.
\usepackage{booktabs}
\usepackage{dcolumn}
\newcommand{\colhead}[1]{\multicolumn{1}{c}{#1}}
\usepackage[labelsep=period]{caption}
\usepackage{subcaption} 
% knitr requires a command for subfig compatibility
\newcommand{\subfloat}[2][need a sub-caption]{\subcaptionbox{#1}{#2}}
\usepackage{graphicx}

% Use proper math notation.
\usepackage{vector}
\renewcommand*{\vec}[1]{\ensuremath{\boldsymbol{\bvec{#1}}}}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\usepackage{xfrac}

% For creating comments
\usepackage{xcolor}
\newcommand{\wrap}[3]{{\small \textcolor{#2}{[\textbf{#1:} #3]}}}
\newcommand{\Bas}[1]{\wrap{Bas}{blue}{#1}}
\newcommand{\Ashley}[1]{\wrap{Ashley}{red}{#1}}
\newcommand{\Johan}[1]{\wrap{Johan}{orange}{#1}}

\title{%
  On comparative statistics for labelling tasks:\\%
  What can we learn from MIREX ACE $\mathbf{2013}$?%
}

% The style file can't handle titles with any formatting.
\renewcommand{\permission}{%
\begin{figure}[b]{%
\footnotesize
%{\includegraphics[height=0.45cm]{figs/88x31}}
{\includegraphics[height=0.45cm]{figs/cc_by.pdf}}
\vskip -.45cm
\begin{spacing}{1.1}
\hskip 1.5cm \copyright \hskip .1cm \authorname.\\
Licensed under a Creative Commons Attribution 4.0 International License (\acro{CC BY} 4.0).
{\bf Attribution: } \authorname.
``On comparative statistics for labelling tasks: What can we learn from \acro{MIREX ACE} 2013?'',
\conferenceedition\ International Society for Music Information Retrieval Conference,  \conferenceyear.
\end{spacing}
}
\end{figure}
}

\threeauthors%
{John Ashley Burgoyne}%
{Universiteit van Amsterdam \\ {\tt j.a.burgoyne@uva.nl}}%
{Bas de Haas}%
{Universiteit Utrecht \\ {\tt w.b.dehaas@uu.nl}}%
{Johan Pauwels}%
{\acro{STMS IRCAM--CNRS--UPMC} \\ {\tt
    johan.pauwels@gmail.com}\thanks{Johan Pauwels is no longer
    affiliated with \acro{STMS}. Data and source code to reproduce
    this paper, including all statistics and figures, are available
    from \protect\url{http://bitbucket.org/jaburgoyne/ismir-2014}.}}
  
\begin{document}
<<global-opts, cache=FALSE, include=FALSE>>=
library(knitr)  
library(car)       # contr.Sum() for readable contrasts
library(geepack)   # geeglm() for GEEs
library(boot)      # inv.logit() 
library(aod)       # vcov.geeglm(): needed for Tukey tests
library(multcomp)  # glht(), cld(), and mcp() for Tukey tests

opts_chunk$set(fig.align="center", cache=TRUE, echo=FALSE, warning=FALSE)

DATA.SETS <- c("MirexChord2009", 
               "BillboardTest2012", "BillboardTest2013")
## Algorithms in order of SeventhsBass performance.
ALGOS <- c("KO2", "NMSD2", "CB4", "NMSD1", "CB3", "KO1", 
           "PP4", "PP3", "CF2", "NG1", "NG2", "SB8")
VOCABS <- c("Root", 
            "MajMin", "MajMinBass", 
            "Sevenths", "SeventhsBass")
ANALYSES <- c(VOCABS, 
              "Segmentation")
DURATIONS <- c(sapply(tolower(VOCABS), 
                      function (x) paste0("Dur", x)),
               list(Segmentation = "DurRoot"))

FormatP <- function(p) {
  #' Format p-values according to APA specifications: 3 decimal places
  #' unless less than .001.
  #'
  ifelse(p < 0.001, "$p <$ .001", paste0("$p =$ .", formatC(1000 * p,
                                                            format = "f",
                                                            digits = 0,
                                                            width  = 3,
                                                            flag   = "0")))
}

FormatNum <- function(x) {
  #' Add thin spaces as thousands separators expect for four-digit integers.
  #'
  ifelse(abs(x) < 10000 && abs(x - round(x)) < .Machine$double.eps ^ 0.5,
         prettyNum(x),
         prettyNum(x, big.mark="\\\\,"))
}
@ 
%
\maketitle
%
\begin{abstract}
  For \acro{MIREX} 2013, the evaluation of audio chord estimation
  \acro{(ACE)} followed a new scheme.  Using chord vocabularies of
  differing complexity as well as segmentation measures, the new
  scheme provides more information than the \acro{ACE} evaluations
  from previous years. With this new information, however, comes new
  interpretive challenges. What are the correlations among different
  songs and, more importantly, different submissions across the new
  measures?  Performance falls off for all submissions as the
  vocabularies increase in complexity, but does it do so directly in
  proportion to the number of more complex chords, or are certain
  algorithms indeed more robust? What are the outliers, song-algorithm
  pairs where the performance was substantially higher or lower than
  would be predicted, and how can they be explained? Answering these
  questions requires moving beyond the Friedman tests that have most
  often been used to compare algorithms to a richer underlying
  model. We propose a logistic-regression approach for generating
  comparative statistics for \acro{MIREX} \acro{ACE}, supported with
  generalised estimating equations \acro{(GEEs)} to correct for
  repeated measures. We use the \acro{MIREX} 2013 \acro{ACE} results
  as a case study to illustrate our proposed method, including some of
  interesting aspects of the evaluation that might not apparent from
  the headline results alone.
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

Automatic chord estimation \acro{(ACE)} has a long tradition within
the music information retrieval \acro{(MIR)} community, and chord
transcriptions are generally recognised as a useful mid-level
representation in academia as well as in industry. For instance, in an
academic context it has been shown that chords are interesting for
addressing musicological hypotheses~\cite{mauch2007,burgoynephd}, and
that they can be used as a mid-level feature to aid in retrieval tasks
like cover-song detection~\cite{haas2011,khadkevich2013}. In an
industrial setting, music start-ups like
Riffstation\footnote{\url{http://www.riffstation.com/}} and
Chordify\footnote{\url{http://chordify.net}} use \acro{ACE} in their
music teaching tools, and at the time of writing, Chordify attracts
more than 2 million unique visitors every month~\cite{haas2012}.

% 2008: 13 teams, 15 algorithms
% 2009: 10 teams, 18 algorithms
% 2010: 10 teams, 15 algorithms
% 2011:  9 teams, 18 algorithms
% 2012:  6 teams, 11 algorithms
% 2013:  7 teams, 12 algorithms

In order to compare different algorithmic approaches in an impartial
setting, the Music Information Retrieval Evaluation eXchange
\acro{(MIREX)} introducted an annual \acro{ACE} task in 2008. Since
then, between 11 and 18 algorithms have been submitted each year by
between 6 and 13 teams. Despite the fact that \acro{ACE} algorithms
are used outside of academic environments, and even though the number
of \acro{MIREX} participants has decreased slightly over the last
three years, the problem of automatic chord estimation is nowhere near
solved. Automatically extracted chord sequences have classically been
evaluated by calculating the \emph{chord symbol recall} \acro{(CSR)},
which reflects the proportion of correctly labelled chords in a single
song, and a \emph{weighted chord symbol recall} \acro{(WCSR)}, which
weights the average \acro{CSR} of a set of songs by their length. On
fresh validation data, the best-performing algorithms in 2013 achieved
\acro{WCSR} of only 75 percent, and that only when the range of
possible chords was restricted exclusively to the 25 major, minor and
``no-chord'' labels; the figure drops to 60 percent when the
evaluation is extended to include seventh chords (see
Table~\ref{tab:mirex-results}).

\begin{table*}
  \small
  \centering
 \begin{tabular}{clcD{.}{}{2.0}ccD{.}{}{2.0}D{.}{}{2.0}D{.}{}{2.0}D{.}{}{2.0}D{.}{}{2.0}D{.}{}{2.0}D{.}{}{2.0}D{.}{}{2.0}}
    \toprule
    \multicolumn{3}{c}{Algorithm} &
    \colhead{\# Types} & \colhead{Inversions?} & \colhead{Training?} &
    \colhead{I} & \colhead{II} & \colhead{III} & \colhead{IV} &
    \colhead{V} & \colhead{VI} & \colhead{VII} & \colhead{VIII} \\
    \midrule
    & \acro{KO2}   & &  7 & & \textbullet & 76 & 74 & 72 & 60 & 58 & 84 & 79 & 89 \\
    & \acro{NMSD2} & & 10 & &             & 75 & 71 & 69 & 59 & 57 & 82 & 79 & 86 \\
    & \acro{CB4}   & & 13 & & \textbullet & 76 & 72 & 70 & 59 & 57 & 85 & 80 & 90 \\
    & \acro{NMSD1} & & 10 & &             & 74 & 71 & 69 & 58 & 56 & 83 & 79 & 86 \\
    & \acro{CB3}   & & 13 & &             & 76 & 72 & 70 & 58 & 56 & 85 & 81 & 89 \\
    & \acro{KO1}   & &  7 & &             & 75 & 71 & 69 & 54 & 52 & 83 & 80 & 88 \\
    & \acro{PP4}   & &  5 & &             & 69 & 66 & 64 & 51 & 49 & 83 & 78 & 87 \\
    & \acro{PP3}   & &  2 & &             & 70 & 68 & 65 & 50 & 48 & 83 & 82 & 84 \\
    & \acro{CF2}   & & 10 & \textbullet & & 71 & 67 & 65 & 49 & 47 & 83 & 83 & 83 \\
    & \acro{NG1}   & &  2 & &             & 71 & 67 & 65 & 49 & 46 & 82 & 79 & 86 \\
    & \acro{NG2}   & &  5 & &             & 67 & 63 & 61 & 44 & 43 & 82 & 81 & 83 \\
    & \acro{SB8}   & &  2 & &             &  9 &  7 &  6 &  5 &  5 & 51 & 92 & 35 \\
    \bottomrule
  \end{tabular}
  \caption{Number of supported chord types, inversion support,
    training support, and \acro{MIREX} results on the
    \emph{Billboard} 2013 test set for all 2013 \acro{ACE}
    submissions. I: root only; II: major-minor
    vocabulary; III: major-minor vocabulary with inversions; IV:
    major-minor vocabulary with sevenths; V: major-minor vocabulary
    with sevenths and inversions; VI: mean segmentation score; VII:
    under-segmentation; VIII: over-segmentation. Adapted from the
    \acro{MIREX} Wiki.}
  \label{tab:mirex-results}
\end{table*}

\acro{MIREX} is a terrific platform for evaluating the performance of
\acro{ACE} algorithms, but by 2010 it was already being recognised
that the metrics could be improved. At that time, they included only
\acro{CSR} and \acro{WCSR} using a vocabulary of 12 major chords, 12
minor chords and a ``no-chord'' label.  At \acro{ISMIR} 2010, a group
of ten researchers met to discuss their dissatisfaction. In the
resulting \enquote{Utrecht Agreement}, it was proposed that future
evaluations should include more diverse chord vocabularies, such as
seventh chords and inversions, as the 25-chord vocabulary was
considered a rather coarse representation of tonal
harmony.\footnote{\url{http://www.music-ir.org/mirex/wiki/The_Utrecht_Agreement_on_Chord_Evaluation}}
Furthermore, the group agreed that it was important to include a
measure of segmentation quality in addition to \acro{CSR} and
\acro{WCSR}.

At approximately the same time, Christopher Harte proposed a
formalisation of measures that implemented the aspirations indicated
in the Utrecht agreement~\cite{harte2010}. Recently, Pauwels and
Peeters reformulated and extended Harte's work with the precise aim of
handling differences in chord vocabulary between annotated ground
truth and algorithmic output on one hand, and among the output of
different algorithms on the other
hand~\cite{Pauwels-Peeters-2013}. They also performed a rigorous
re-evaluation of all \acro{MIREX} \acro{ACE} submissions from 2010 to
2012. As of \acro{MIREX} 2013, these revised evaluation procedures,
including the chord sequence segmentation evaluation suggested by
Harte~\cite{harte2010} and Mauch~\cite{mauch2010}, have been adopted
in the context of the \acro{MIREX} \acro{ACE}
task.%\footnote{\url{http://www.music-ir.org/mirex/wiki/2013:Audio_Chord_Estimation}}

\acro{MIREX} \acro{ace} evaluation has also typically included
comparative statistics to help determine whether the differences in
performance between pairs of algorithms are statistically
significant. Traditionally, Friedman's \acro{ANOVA} has been used for
this purpose, accompanied by Tukey's Honest Significant Difference
tests for each pair of algorithms. Friedman's \acro{ANOVA} is
equivalent to a standard two-way \acro{ANOVA} with the actual
measurements (in our case \acro{WCSR} or directional Hamming distance
[\acro{DHD}], the new segmentation measure) %
%% \Bas{In this paper we refer to ``the new segmentation measure'' or
%%   something similar. I think it would be a good idea to give this new
%%   segmentation measure a proper name. Maybe we can just call it
%%   Directional Hamming Distance (\acro{DHD}?)}
%% \Johan{Good point, a quick search learns me that DHD seems to be the
%%   already established name and it's not that new in fact. It comes
%%   from the image segmentation field and has been first used for music
%%   in Abdallah et al. 2005 ISMIR paper. See also
%%   \url{http://ismir2008.ismir.net/papers/ISMIR2008_219.pdf}, we'd
%%   better also reattribute the citations on line 216.}
%% \Ashley{I don't follow what you mean by reattribution here, Johan.}
replaced by the rank of each treatment (in our case, each algorithm)
on that measure within each block (in our case, for each song)
\cite{Kutner2005}. The rank transformation makes Friedman's
\acro{ANOVA} an excellent \enquote{one size fits all} approach that
can be applied with minimal regard to the underlying distribution of
the data, but these benefits come with costs. Like any non-parametric
test, Friedman's \acro{ANOVA} can be less powerful than parametric
alternatives where the distribution is known, and the rank
transformation can obscure information inherent to the underlying
measurement, magnifying trivial differences and neutralising
significant inter-correlations.

But there is no need to pay the costs of Friedman's \acro{ANOVA} for
evaluating chord estimation. Fundamentally, \acro{WCSR} is a
proportion, specifically the expected proportion of audio frames that
an estimation algorithm will label correctly, and as such, it fits
naturally into \emph{logistic regression} (i.e., a \emph{logit
  model}). Likewise, \acro{DHD} is constrained to fall
between 0 and 100 percent, and thus it is also suitable for the same
type of analysis. The remainder of this paper describes how logistic
regression can be used to compare chord estimation algorithms, using
\acro{MIREX} results from 2013 to illustrate four key benefits: easier
interpretation, greater statistical power, an inter-correlation for
identifying relationships among submitted algorithms, and better
detection of outliers.

\section{Logistic Regression with GEEs}\label{sec:logistic-regression}

Proportions cannot be distributed normally because they are supported
exclusively on [0, 1], and thus they present challenges for
traditional techniques of statistical analysis. Logit models are
designed to handle these challenges without sacrificing the simplicity
of the usual linear function relating parameters and covariates
\cite[ch.~4]{Agresti2007}:
\begin{equation}
  \label{eq:logit1}
  \pi(\vec{x}; \vec{\betaup}) = 
  \frac{\mathrm{e}^{\vec{x}'\vec{\betaup}}}
  {1 + \mathrm{e}^{\vec{x}'\vec{\betaup}}}
  \ ,
\end{equation}
or equivalently
\begin{equation}
  \label{eq:logit2}
  \log \frac{\pi(\vec{x}; \vec{\betaup})}{1-\pi(\vec{x}; \vec{\betaup})} = \vec{x}'\vec{\betaup}
  \ ,
\end{equation}
where $\pi$ represents the relative frequency of \enquote{success}
given the values of covariates in $\vec{x}$ and parameters
$\vec{\betaup}$. In the case of a basic model for \acro{MIREX}
\acro{ACE}, $\vec{x}$ would identify the algorithm and $\pi$ would be
the relative frequency of correct chord labels for that algorithm
(i.e., \acro{WCSR}). In the case of data like \acro{ACE} results,
where there are proportions $p_i$ of correct labels over $n_i$
analysis frames rather than binary successes or failures ($i$ indexing
all combinations of individual songs and algorithms), logistic
regression assumes that each $p_i$ represents the observed proportion
of successes among $n_i$ conditionally-independent binary
observations, or more formally, that the $p_i$ are distributed
binomially:
\begin{equation}
  \label{eq:binomial}
  f_{P \mid N, \vec{X}}(p \mid n, \vec{x}; \vec{\betaup}) = 
  \binom{n}{p n}\, 
  \pi^{p n} 
  (1 -\pi)^{(1 - p) n}
  \ .
\end{equation}
The expected value for each $p_i$ is naturally $\pi_i = \pi(\vec{x}_i; \vec{\betaup})$ the overall relative frequency of success given
$\vec{x}_i$:
\begin{equation}
  \label{eq:E}
  \mathbf{E}\left[P \mid N, \vec{X}\right] = \pi(\vec{x}; \vec{\betaup})\ .
\end{equation}

Logistic regression models are most often fit by the maximum-likelihood
technique, i.e., one is seeking a vector $\vec{\hat{\betaup}}$ to
maximise the log-likelihood given the data:
\begin{multline}
  \label{eq:loglik}
  \ell_{P \mid N, \vec{X}} (\vec{\betaup}; \vec{p}, \vec{n}, \vec{X}) =
  \sum_i \left[
    \log \binom{n_i}{p_i n_i}\ + \right. \\ \left.\phantom{\binom{x}{x}}
    p_i n_i \log \pi_i + 
    (1 - p_i) n_i \log\,(1 -\pi_i)
  \right]\ .
\end{multline}
One thus solves the system of likelihood equations for
$\vec{\betaup}$, whereby the gradient of Equation~\ref{eq:loglik} is
set to zero:
\begin{equation}
  \label{eq:gradient}
  \nabla_{\vec{\betaup}}
  \ell_{P \mid N, \vec{X}} 
  (\vec{\betaup}; \vec{p}, \vec{n}, \vec{X}) =
  \sum_i (p_i - \pi_i) n_i \vec{x}_i =
  \vec{0}
\end{equation}
and so
\begin{equation}
  \label{eq:betahat}
  \sum_i p_i n_i \vec{x}_i = 
  \sum_i \pi_i n_i \vec{x}_i\ .
\end{equation}
In the case of \acro{MIREX} \acro{ACE} evaluation, each $\vec{x}_i$ is simply an
indicator vector to partition the data by algorithm, and thus
$\vec{\hat{\betaup}}$ is the parameter vector for which
$\pi_i$ equals the song-length--weighted mean over all $p_i$
for that algorithm.

\subsection{Quasi-Binomial Models}

Under a strict logit model, the variance of each $p_i$ is inversely
proportional to $n_i$:
\begin{equation}
  \label{eq:var}
  \var\left[P \mid N, \vec{X}\right] = 
  \left(\frac{1}{n}\right) \pi (1 - \pi)\ .
\end{equation}
Equation~\ref{eq:var} only holds, however, if the estimates of chord labels
for each audio frame are independent. For \acro{ACE}, this is
unrealistic: only the most naïve algorithms treat every frame
independently; some kind of time-dependence structure is standard,
most frequently a hidden Markov model or some close derivative
thereof. Hence one should expect that the variance of \acro{WCSR}
estimates would be rather larger than the basic logit model would
suggest. 

This type of problem is extremely common across disciplines, so much
so that is has been given a name, \emph{over-dispersion}, and some
authors go so far as to state that \enquote{unless there are good
  external reasons for relying on the binomial assumption [of
  independence], it seems wise to be cautious and to assume that
  over-dispersion is present to some extent unless and until it is
  shown to be absent} \cite[p.~125]{McCullagh1989}.  One standard
approach to handling over-dispersion is to use a so-called
\emph{quasi-likelihood} \cite[\S\,4.7]{Agresti2007}. In case of
logistic regression, this typically entails a modification to the
assumption on the distribution of the $p_i$ that includes an
additional \emph{dispersion parameter} $\phi$. The expected values are
the same as a standard binomial model, but
\begin{equation}
  \label{eq:quasi-var}
  \var\left[P \mid N, \vec{X}\right] = 
  \left(\frac{\phi}{n}\right) \pi (1 - \pi)\ .
\end{equation}

These models are known as quasi-likelihood models because one loses a
closed-form solution for the actual probability distribution $f_{P
  \mid N, \vec{X}}$; one knows only that the $p_i$ behave something
like binomially-distributed variables, with identical means but
proportionally more variance. The parameter estimates
$\vec{\hat{\betaup}}$ and predictions $\pi(\cdot;
\vec{\hat{\betaup}})$ for a quasi-binomial model are the same as
ordinary logistic regression, but the estimated variance-covariance
matrices are scaled by the estimated dispersion parameter $\hat{\phi}$
(and likewise the standard errors are scaled by its square root). The
dispersion parameter is estimated so that the theoretical variance
matches the empirical variance in the data, and because of the form of
Equation~\ref{eq:quasi-var}, it renders any scaling considerations for
the $n_i$ moot.

Other approaches to handling over-dispersion include
\emph{beta-binomial models} \cite[\S\,13.3]{Agresti2007} and
\emph{beta regression} \cite{Ferrari-Cribari-Neto-2004}, but we prefer
the simplicity of the quasi-likelihood model.

\subsection{Generalised Estimating Equations (\acro{GEE}s)}\label{subsec:GEEs}

The quasi-binomial model achieves most of what one would be looking
for when evaluating \acro{ACE} for \acro{MIREX}: it handles
proportions naturally, is consistent with the weighted averaging used
to compute \acro{WCSR}, and adjusts for over-dispersion in a way that
also eliminates any worries about scaling. Nonetheless, it is slightly
over-conservative for evaluating \acro{ACE}. As discussed earlier,
quasi-binomial models are necessary to account for over-dispersion,
and one important source of over-dispersion in these data is the lack
of independence of chord estimates from most algorithms within the
same song. \acro{MIREX} exhibits another important violation of the
independence assumption, however: all algorithms are tested on the
same sets of songs, and some songs are clearly more difficult than
others. Put differently, one does not expect the algorithms to perform
completely independently of one another on the same song but rather
expects a certain correlation in performance across the set of
songs. By taking that correlation into account, one can improve the
precision of estimates, particularly the precision of pair-wise
comparisons \cite[\S\,10.1]{Agresti2007}.

<<read-results>>=
ReadResults <- function (analysis) {
  #' MIREX results for a particular chord vocabulary or segmentation
  #' 
  #' Assumes that the current directory is that of a particular database in
  #' Johan's distribution format.
  #'
  #' @param analysis chord vocabulary or segmentation to load
  #'
  is.segmentation <- analysis == "Segmentation"
  col.names <- c()
  col.classes <- c()
  if (is.segmentation) {
    setwd("resultsSegmentation")
    col.names <- c("song", "", "under.segmentation", "over.segmentation")
    col.classes <- c("factor", "NULL", "numeric", "numeric")
  } else {
    setwd(paste0("resultsMirex", analysis))
    col.names <- c("song", 
                   "performance", 
                   "duration",
                   rep("", 6))
    col.classes <- c("factor", "numeric", "numeric", rep("NULL", 6))
  }
  results <- data.frame()
  for (algo in ALGOS) {
    algo.results <- read.table(paste0(algo, ".csv"),
                               skip = 2,
                               header = FALSE,
                               sep = ",",
                               dec = ".",
                               col.names = col.names, 
                               colClasses = col.classes)
    if (is.segmentation) {
      ## Replace over- and under-segmentation with the 
      ## harmonic mean of their arithmetic inverses. 
      algo.results$performance <-
        1 / (0.5 * (1/(1-algo.results$under.segmentation) + 
                    1/(1-algo.results$over.segmentation)))
      algo.results$over.segmentation <- c()
      algo.results$under.segmentation <- c()
    } else {
      ## Rescale RCO results back to [0, 1].
      algo.results$performance <- algo.results$performance/100
    }
    results <- rbind(results, 
                     cbind(algorithm = rep(algo, dim(algo.results)[1]),
                           algo.results))
  }
  setwd("..")
  if (is.segmentation) results$duration <- ReadResults("Root")$duration
  results <-
    within(results, {
      performance.rank <- rep(NA, length(performance))
      for (group in levels(song))
        performance.rank[song==group] <- rank(performance[song==group])
    })
  return(results)
}

ReadAllResults <- function (data.set) {
  #' Complete battery of MIREX results
  #' 
  #' Assumes that the current directory contains directories of results
  #' for each database.
  #' 
  #' @param data.set the data set for which to load evaluations
  #'
  setwd(data.set)
  results <-
    within(
      Reduce(rbind,
             lapply(ANALYSES, 
                    function (analysis) {
                      result <- ReadResults(analysis)
                      result$analysis <- rep(analysis, dim(result)[1])
                      return(result)
                    }),
             data.frame()),
      {
        analysis <- factor(analysis, levels=ANALYSES)
        vocabulary <- analysis
        levels(vocabulary) <- c("Root", "MajMin", "MajMin", "Sevenths", "Sevenths", NA)
        bass <- analysis == "MajMinBass" | analysis == "SeventhsBass"
        bass[analysis == "Segmentation"] <- NA
        ## Order the algorithm factor for easier comparisons.
        algorithm <- factor(algorithm, levels=ALGOS)
     })
  setwd("..")
  return(results[order(results$song),]) # group sorting for geeglm()
}

# Read subsets for this paper and set contrasts.
billboard.2013 <- ReadAllResults("BillboardTest2013")
contrasts(billboard.2013$algorithm) <- contr.Sum(ALGOS)
billboard.2013.7b <-
  droplevels(billboard.2013[billboard.2013$analysis == "SeventhsBass",])
contrasts(billboard.2013.7b$algorithm) <- contr.Sum(ALGOS)
@ 

<<cld-summaries, dependson="read-results">>=
friedman.summary <- with(billboard.2013.7b,
  summary(glht(glm(formula   = performance.rank ~ song + algorithm,
                   weights   = duration),
               mcp(algorithm = "Tukey")),
          test = adjusted("fdr")))
logistic.summary <- with(billboard.2013.7b,
  summary(glht(geeglm(formula   = performance ~ algorithm,
                      family    = binomial(link = "logit"),
                      corstr    = "exchangeable",
                      weights   = duration,
                      id        = song),
               mcp(algorithm = "Tukey")),
          test = adjusted("fdr")))
@ 

<<cld-plots, dependson=c("cld-summaries"), fig.show="hold", fig.width=7, fig.height=5, include=FALSE>>=
## Compact letter displays require an extra top margin.
par(mar = par("mar") + c(0, 0, 5, 0))
friedman.cld <- cld(friedman.summary, level=.005)
friedman.cld$covar <- FALSE # Tricks plot.cld into using the response scale.
plot(friedman.cld,
     xlab = "Algorithm",
     ylab = "Rank per Song (1 low; 12 high)",
     las  = 2)
points(1:12,
       coef(friedman.summary$model)[1] +
       friedman.summary$model$contrasts$algorithm %*%
       coef(friedman.summary$model)[162:172],
       pch = 16)
plot(cld(logistic.summary, level=.005),
     xlab = "Algorithm",
     ylab = "Chord-Symbol Recall (CSR)",
     las  = 2)
points(1:12,
       inv.logit(coef(logistic.summary$model)[1] +
                 logistic.summary$model$contrasts$algorithm %*%
                 coef(logistic.summary$model)[2:12]),
       pch = 16)
@ 

\begin{figure*}
  \centering
  \subfloat[Friedman's \acro{ANOVA}\label{fig:cld-plots1}]{
    \centering \includegraphics[width=0.45\maxwidth]{figure/cld-plots1} 
  }
  \quad
  \subfloat[Logistic Regression\label{fig:cld-plots2}]{
    \centering \includegraphics[width=0.45\maxwidth]{figure/cld-plots2} 
  }
  \caption{Boxplots and compact letter displays for the \acro{MIREX}
    \acro{ACE} 2013 results on the \emph{Billboard} 2013 test set with
    vocabulary V (seventh chords and inversions). Bold lines represent
    medians and filled dots (weighted) means. $N =$
    \Sexpr{length(levels(billboard.2013[['song']]))} songs per
    algorithm. Given the respective statistical models, there are
    insufficient data to distinguish among algorithms sharing a
    letter, correcting to hold the \acro{FDR} at $\alpha =$
    .005. Although Friedman's \acro{ANOVA} detects slightly more
    significant pairwise differences than logistic regression
    (\Sexpr{sum(friedman.summary$test$pvalues < .005)}
    vs. \Sexpr{sum(logistic.summary$test$pvalues < .005)}), it
    operates on a different scale than \acro{CSR} and misorders
    algorithms relative to \acro{WCSR}.}
  \label{fig:cld-plots}  
\end{figure*}

A relatively straightforward variant of quasi-likelihood known as
\emph{generalised estimating equations} (\acro{GEE}s) incorporates
this type of correlation \cite[ch.~11]{Agresti2007}. With the
\acro{GEE} approach, rather than predicting each $p_i$ individually,
one predicts complete vectors of proportions $\vec{p}_i$ for each
relevant group, much as Friedman's test seeks to estimate ranks within
each group. For \acro{ACE}, the groups are songs, and thus one
considers the observations to be vectors $\vec{p}_i$, one for each
song, where $p_{ij}$ represents the \acro{CSR} or segmentation score
for algorithm $j$ on song $i$. Analogous to the case of ordinary
quasi-binomial or logistic regression,
\begin{equation}
  \label{eq:gee-E}
  \mathbf{E}\left[P_j \mid N, \vec{X}_j\right] =
  \pi(\vec{x}_j; \vec{\betaup})\ .   
\end{equation}
Likewise, analogous to the quasi-binomial variance,
\begin{equation}
  \label{eq:gee-var}
  \var\left[P_j \mid N, \vec{X}_j\right] =
  \left(\frac{\phi}{n}\right) \pi_j (1 - \pi_j)\ .
\end{equation}

Because the \acro{GEE} approach is concerned with vector-valued estimates
rather than point estimates, it also involves estimating a full
variance-covariance matrix. In addition to $\vec{\betaup}$ and $\phi$,
the approach requires a further vector of parameters $\vec{\alphaup}$
and an \emph{a priori} assumption on the correlation structure of the
$P_j$ in the form of a function $R(\vec{\alphaup})$ that yields a
correlation matrix. (One might, for example, assume that that the
$P_j$ are \emph{exchangeable}, i.e., that every pair shares a common
correlation coefficient.) Then if $B$ is a diagonal matrix such that
$B_{jj} = \var\,[P_j \mid N, \vec{X}_j]$,
\begin{equation}
  \label{eq:gee-cov}
  \cov\left[\vec{P} \mid N, \vec{X}\right] =
  B^{\sfrac{1}{2}} R(\vec{\alphaup}) B^{\sfrac{1}{2}}\ .
\end{equation}
If all of the $P_j$ are uncorrelated with each other, then this
formula reduces to the basic quasi-binomial model, which assumes a
diagonal covariance matrix. The final step of \acro{GEE} estimation
adjusts Equation~\ref{eq:gee-cov} according to the actual correlations
observed in the data, and as such, \acro{GEE}s are quite robust in
practice even when the \emph{a priori} assumptions about the
correlation structure are incorrect \cite[\S\,11.4.2]{Agresti2007}.

\section{Illustrative Results}

\acro{MIREX} \acro{ACE} 2013 evaluated 12 algorithms according to a
battery of eight rubrics (\acro{WCSR} on five harmonic vocabularies
and three segmentation measures) on each of three different data sets
(the Isophonics set, including music from the Beatles, Queen, and
Zweieck \cite{mauch2010} and two versions of the McGill
\emph{Billboard} set, including music from the American pop charts
\cite{burgoyne2011}). There is insufficient space to present the
results of logistic regression on all combinations, and so we will
focus on a single one of the data sets, the \emph{Billboard} 2013 test
set. In some cases, logistic regression allows us to speak to all
measures (\Sexpr{FormatNum(dim(billboard.2013)[1])} observations), but
in general, we will also restrict ourselves to discussing the newest
and most challenging of the harmonic vocabularies for \acro{WCSR}:
Vocabulary V (\Sexpr{FormatNum(dim(billboard.2013.7b)[1])}
observations), which includes major chords, minor chords, major
sevenths, minor sevenths, dominant sevenths, and the complete set of
inversions of all of the above. We are interested in four key
questions.
\begin{enumerate}
\item How do pairwise comparisons under logistic regression compare to
  pairwise comparisons with Friedman's \acro{ANOVA}?  Is logistic
  regression more powerful?
\item Are there differences among algorithms as the harmonic
  vocabularies get more difficult, or is the drop performance uniform?
  In other words, is there a benefit to continuing with so many
  vocabularies?
\item Are all \acro{ACE} algorithms making similar mistakes, or do
  they vary in their strengths and weaknesses?
\item Which algorithm-song pairs exhibited unexpectedly good or
  bad performance, and is there anything to be learned
  from these observations?
\end{enumerate}

% \Bas{Very Nice!}
% \Ashley{Thanks!}

\subsection{Pairwise Comparisons}

The boxplots in Figure~\ref{fig:cld-plots} give a more detailed view
of the performance of each algorithm than
Table~\ref{tab:mirex-results}. The figure is restricted to Vocabulary
V, with the algorithms in descending order by
\acro{WCSR}. Figure~\ref{fig:cld-plots1} comes from the traditional
Friedman's \acro{ANOVA}, and thus its $y$-axis reflects not \acro{CSR}
directly but the per-song ranks with respect to
\acro{CSR}. Figure~\ref{fig:cld-plots2} comes from quasi-binomial
regression estimated with \acro{GEE}s, as described in
Section~\ref{sec:logistic-regression}. Its $y$-axis reflects per-song
\acro{CSR} directly. Above the boxplots, all significant pairwise
differences are recorded as a \emph{compact letter display}. In the
interest of reproducible research, we used a stricter $\alpha =$ .005
threshold for reporting pairwise comparisons with the more
contemporary false-discovery-rate \acro{(FDR)} approach of Benjamini
and Hochberg as opposed to more traditional Tukey tests at $\alpha =$
.05 \cite{Benjamini-Hochberg-1995,Johnson-2013}. Within either of the
subfigures, the difference in performance between two algorithms that
share any letter in the compact letter display is \emph{not}
statistically significant. Overall, Friedman's \acro{ANOVA} found
\Sexpr{sum(friedman.summary$test$pvalues < .005) -
  sum(logistic.summary$test$pvalues < .005)} more significant pairwise
differences than logistic regression.

<<correlations, dependson="read-results">>=
PCorr <- function(m, n, adjust = NULL) {
  #' Replaces upper triangle of a correlation matrix with p-values, corrected for
  #' multiple comparisons.
  #'
  #' @param m the correlation matrix
  #' @param n the number of pairs for each entry in the matrix
  #' @param adjust the adjustment method for multiple comparisons with p.adjust
  #'
  upper <- row(m) < col(m)
  m[upper] <- m[upper] * sqrt((n - 2) / (1 - m[upper] ^ 2))
  m[upper] <- pt(abs(m[upper]), n - 2, lower.tail = FALSE) * 2
  m[upper] <- p.adjust(m[upper], adjust)
  m
}

WCorr <- function (a, b = a, w = rep(1, nrow(a))/nrow(a)) 
  #' Pearson's correlation matrix with weighted pairs. Adapted from
  #' http://stackoverflow.com/questions/9460664/weighted-pearsons-correlation.
  #'
  #' See also: corr() in the boot package
  #'
{
  ## Convert to matrices.
  if (is.data.frame(a)) a <- as.matrix(b)
  if (is.data.frame(b)) b <- as.matrix(b)

  ## Normalize weights.
  w <- w / sum(w)

  ## Centre matrices.
  a <- sweep(a, 2, colSums(a * w))
  b <- sweep(b, 2, colSums(b * w))

  ## Compute weighted correlation.
  t(w*a) %*% b / sqrt( colSums(w * a**2) %*% t(colSums(w * b**2)) )
}

logistic.cor <- with(billboard.2013.7b, {
  .contrasts. <- logistic.summary$model$contrasts$algorithm
  .vcov. <- vcov(logistic.summary$model)
  cov2cor(.contrasts. %*% .vcov.[2:12,2:12] %*% t(.contrasts.))
})
logistic.cor <- PCorr(logistic.cor, nlevels(billboard.2013.7b$song), "fdr")

friedman.cor <- with(billboard.2013.7b, {
  .ranks. <- reshape(data      = data.frame(song, algorithm, performance.rank, duration),
                     idvar     = c("song"),
                     timevar   = c("algorithm"),
                     direction = "wide")
  rownames(.ranks.) <- .ranks.[, 1]
  .ranks. <- .ranks.[, c(seq(2, 24, 2), 25)]
  colnames(.ranks.) <- c(ALGOS, "duration")
  WCorr(.ranks.[, 1:12], w = .ranks.[, 13])
})
friedman.cor <- PCorr(friedman.cor, nlevels(billboard.2013.7b$song), "fdr")
@ 

\begin{table*}
  \small
  \centering  
  \begin{tabular}{clcD{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}}
    \toprule 
    \multicolumn{3}{c}{Algorithm} &
    \colhead{\acro{KO2}} & \colhead{\acro{NMSD2}} &
    \colhead{\acro{CB4}} & \colhead{\acro{NMSD1}} &
    \colhead{\acro{CB3}} & \colhead{\acro{KO1}} &
    \colhead{\acro{PP4}} & \colhead{\acro{PP3}} &
    \colhead{\acro{CF2}} & \colhead{\acro{NG1}} &
    \colhead{\acro{NG2}} & \colhead{\acro{SB8}}\\
    \midrule
    & \acro{KO2} &&   \multicolumn{1}{c}{--}&
                                   .07      &  .11      & -.05      &  .10      &  .03      & -.41^\ast & -.44^\ast & -.03      & -.35^\ast &  .05      & -.01\\
    & \acro{NMSD2} &&  .25^\ast & \multicolumn{1}{c}{--}&
                                              -.01      &  .49^\ast & -.25^\ast & -.20      & -.19      & -.36^\ast &  .00      & -.33^\ast &  .02      & -.06\\
    & \acro{CB4}   &&  .41^\ast &  .39^\ast & \multicolumn{1}{c}{--}&
                                                           .12      &  .47^\ast & -.46^\ast & -.30^\ast & -.48^\ast &  .09      & -.38^\ast &  .08      & -.09\\
    & \acro{NMSD1} &&  .30^\ast &  .60^\ast &  .53^\ast & \multicolumn{1}{c}{--}&
                                                                      -.17      & -.45^\ast & -.08      & -.45^\ast &  .27^\ast & -.44^\ast &  .17      & -.10\\
    & \acro{CB3} &&    .34^\ast &  .10      &  .76^\ast &  .42^\ast & \multicolumn{1}{c}{--}&
                                                                                  -.19      & -.26^\ast & -.14      & -.08      & -.17      & -.16      & -.08\\
    & \acro{KO1} &&   -.04      & -.42^\ast & -.51^\ast & -.51^\ast & -.29^\ast & \multicolumn{1}{c}{--}&
                                                                                              -.10      &  .42^\ast & -.41^\ast &  .50^\ast & -.52^\ast &  .05\\
    & \acro{PP4} &&   -.22      &  .08      & -.16      &  .06      & -.07      & -.05      & \multicolumn{1}{c}{--}&
                                                                                                           .37^\ast & -.03      &  .00      &  .05      & -.03\\
    & \acro{PP3} &&   -.49^\ast & -.46^\ast & -.61^\ast & -.53^\ast & -.37^\ast &  .68^\ast &  .22      & \multicolumn{1}{c}{--}&
                                                                                                                      -.48^\ast &  .66^\ast & -.48^\ast &  .04\\
    & \acro{CF2} &&    .09      &  .19      &  .24^\ast &  .42^\ast &  .17      & -.49^\ast &  .06      & -.51^\ast & \multicolumn{1}{c}{--}&
                                                                                                                                  -.48^\ast &  .48^\ast & -.14\\
    & \acro{NG1} &&   -.54^\ast & -.42^\ast & -.60^\ast & -.56^\ast & -.41^\ast &  .68^\ast &  .04      &  .85^\ast & -.47^\ast & \multicolumn{1}{c}{--}&
                                                                                                                                              -.40^\ast & -.10\\
    & \acro{NG2} &&    .09      &  .17      &  .17      &  .16      & -.03      & -.50^\ast & -.09      & -.54^\ast &  .50^\ast & -.40^\ast & \multicolumn{1}{c}{--}&
                                                                                                                                                          -.11\\
    & \acro{SB8} &&   -.32^\ast & -.44^\ast & -.44^\ast & -.52^\ast & -.46^\ast &  .00      & -.32^\ast &  .08      & -.33^\ast &  .08      & -.16      & \multicolumn{1}{c}{--}\\
    \bottomrule
  \end{tabular}
  \caption{Pearson's correlations on the coefficients from
    logistic regression \acro{(WCSR)} for the \emph{Billboard}
    2013 test set with vocabulary V (lower triangle); Spearman's
    correlations for the same data (upper triangle). $N =$ 161 songs per cell. Starred
    correlations are significant at $\alpha =$ .005, controlling for
    the \acro{FDR}. A set of algorithms (viz.,
    \acro{KO1}, \acro{PP3}, \acro{NG1}, and \acro{SB8}) stands out for
    negative correlations with the top performers; in general,
    these algorithms did not attempt to recognise seventh chords.}
  \label{tab:cor}
\end{table*}

\subsection{Effect of Vocabulary}

<<vocabulary, dependson="read-results">>=
friedman.vocab.anova <-
  with(droplevels(billboard.2013[billboard.2013$analysis != "Segmentation",]),
       Anova(glm(performance.rank ~ song + algorithm * vocabulary * bass,
                 weights = duration),
             test.statistic = "F"))
logistic.vocab.anova <-
  with(droplevels(billboard.2013[billboard.2013$analysis != "Segmentation",]),
       anova(geeglm(performance ~ algorithm * (vocabulary + bass), 
                    family = binomial(link="logit"),
                    corstr = "exchangeable",
                    weights = duration,
                    id = song),
             geeglm(performance ~ algorithm * analysis, 
                    family = binomial(link="logit"),
                    corstr = "exchangeable",
                    weights = duration,
                    id = song)))
@ 

To test the utility of the new evaluation vocabularies, we ran both
Friedman \acro{ANOVA}s (ranked separately for each vocabulary) and
logistic regressions and looked for significant interactions among
the algorithm, inversions (present or absent from the vocabulary)
and the complexity of the vocabulary (root only, major-minor, or
major-minor with 7ths). Under Friedman's \acro{ANOVA}, there was a
significant Algorithm $\times$ Complexity interaction,
$F($\Sexpr{friedman.vocab.anova[5,2]},
\Sexpr{sum(friedman.vocab.anova[9,2])}$) =$
\Sexpr{round(friedman.vocab.anova[5,3], 2)},
\Sexpr{FormatP(friedman.vocab.anova[5,4])}. The logistic regression
model identified a significant three-way Algorithm $\times$ Complexity
$\times$ Inversions interaction,
$\chi^2($\Sexpr{logistic.vocab.anova[1,1]}$) =$
\Sexpr{round(logistic.vocab.anova[1,2], 2)},
\Sexpr{FormatP(logistic.vocab.anova[1,3])}, but the additional
interaction with inversions should be interpreted with care: only one
algorithm (\acro{CF2}) attempts to recognise inversions.

\subsection{Correlation Matrices}

Table~\ref{tab:cor} presents the inter-correlations of \acro{WCSR}
between algorithms, rank-transformed (Spearman's correlations,
analogous to Friedman's \acro{ANOVA}) in the upper triangle, and in
the lower triangle, as estimated from logistic regression with
\acro{GEE}s. Significant correlations are marked, again controlling
the \acro{FDR} at $\alpha =$ .005. Positive correlations do not
necessarily imply that the algorithms perform similarly; rather it
implies that they find the same songs relatively easy or
difficult. Negative correlations imply that songs that one algorithm
finds difficult are relatively easy for the other algorithm.

\subsection{Outliers}

<<outliers, dependson="read-results">>=
## Chauvenet's criterion for this sample size.
chauvenet <- qnorm(1/(2*dim(billboard.2013)[1]), lower.tail=FALSE)
friedman.outliers <- with(billboard.2013, {
  friedman.fit <- glm(performance.rank ~ song + algorithm * analysis)
  billboard.2013[abs(resid(friedman.fit)) > chauvenet * sd(resid(friedman.fit)),]
})
logistic.outliers <- with(billboard.2013, {
  logistic.fit <- geeglm(formula = performance ~ algorithm * analysis,
                         family = binomial(link="logit"),
                         corstr = "exchangeable",
                         weights = duration, id = song)
  billboard.2013[abs(resid(logistic.fit)) > chauvenet * sd(resid(logistic.fit)),]
})
@ 

To check for outliers, we considered all evaluations on the
\emph{Billboard} 2013 test set. Chauvenet's criterion for outliers in
a sample of this size is to lie more than \Sexpr{round(chauvenet, 2)}
standard deviations from the mean. 
\Bas{A reference would be nice here.}
No data point met this criterion
under Friedman's \acro{ANOVA}. 
\Bas{This is what you would expect, right? After all, outliers 
are removed by the ranking process.}
The most extreme residuals occur for
algorithm \acro{SB8}, on songs that were so difficult for most
algorithms that the essentially random approach of \acro{SB8}, due to
its software bug, did better. Under the logistic regression model,
Chauvenet's criterion identified \Sexpr{dim(logistic.outliers)[1]}
extreme data points. These were primarily for songs that are tuned a
quarter-tone off from standard tuning (A4 = 440\,Hz). The ground truth
necessarily is \enquote{rounded off} to standard tuning in one
direction or the other, but in cases where an otherwise
high-performing algorithm happened to round off in the opposite
direction, the performance is markedly low.

\section{Discussion}

We were surprised to find that strictly in terms of distinguishing
between algorithms, Friedman's \acro{ANOVA} was in fact more powerful
than logistic regression, detecting a few extra significant pairs. A
look at the boxplots in Figure~\ref{fig:cld-plots2}, however, causes
one to question the scientific utility of that power. \acro{SB8}
excepted, which was a buggy submission that erroneously returned
alternating C- and B-major chords regardless of the song, all of the
algorithms performed fairly similarly in absolute terms. Although
further testing might be warranted, it seems that the ranking step may
-- for this task -- exaggerate slight differences in performance that would
be inconsequential for most applications. Furthermore, having now
benefited from years of study, \acro{WCSR} is a reasonably intuitive and
well-motivated measure of \acro{ACE} performance, and it is awkward to
have to work on the Friedman's rank scale instead, especially since it
ultimately ranks the algorithms' overall performance in a slightly
different order than the headline \acro{WCSR}-based results.\Johan{Why is this?}

Friedman's \acro{ANOVA} did exhibit substantially less power for our
question about interactions between algorithms and differing chord
vocabularies. Again, \acro{WCSR} as a unit and as a concept is highly
meaningful for chord estimation, and there is a real conceptual loss
from rank transformation on these types of questions. Given the rank
transformation, Friedman's \acro{ANOVA} can only be sensitive to
reconfigurations of relative performance as the vocabularies become
more difficult; logistic regression can also be sensitive to different
effect sizes across algorithms even when their relative ordering
remains the same.

It was encouraging to see that under either statistical model, there
was a benefit to evaluating with multiple vocabularies. That
encouraged us to dig deeper into an actual correlation matrix of
residuals, only available from logistic regression in this
case. Figure~\ref{fig:hclust-plot} summarises the original correlation
matrix in Table~\ref{tab:cor} more visually by using the correlations
as the basis of a hierarchical clustering. Two clear groups emerge,
both from the clustering and from minding negative correlations in the
original matrix: one relatively low-performing group including
\acro{KO1}, \acro{PP3}, \acro{NG1}, and \acro{SB8}, and one relatively
high-performing group including all others but for perhaps \acro{PP4},
which does not seem to correlate strongly with any other
algorithm. Table~\ref{tab:mirex-results} uncovers the secret behind
the low performance: \acro{KO1} excepted, none of the low-performing
algorithms attempt to recognise seventh chords, which comprise 29~percent
of all chords under Vocabulary V. Furthermore, we performed an additional evaluation of seventh chords only, in the style of~\cite{Pauwels-Peeters-2013} and using their software available online\footnote{\url{https://github.com/jpauwels/MusOOEvaluator}}. From the resulting low score of \acro{KO1}, we can deduce that this algorithm is able to
recognise seventh chords in theory, but that it was mostly likely trained on
the relatively seventh-poor Isophonics corpus (only 15 percent of all
chords). On the other hand, \acro{KO2} is the same algorithm trained locally on an
independent tranche of the \emph{Billboard} corpus, and with that
training, it becomes a top performer. 

<<hclust-plot, dependson="correlations", fig.width=7, fig.height=3.5, fig.out="\\linewidth", fig.cap="Hierarchical clustering of algorithms based on \\acro{WCSR} for for the \\emph{Billboard} 2013 test set with vocabulary V, Pearson's distance, and complete linkage. The group of algorithms that is negatively correlated with the top performers appears at the left. \\acro{PP4} stands out as the most idiosyncratic performer.">>=
par(mar=par("mar")-c(4,0,2,0))
plot(hclust(as.dist((1 - logistic.cor)/2), method="complete"),
            main="", sub="",
            ylab="Pearson's Distance", xlab="")
@ 

Our analysis of outliers again showed Friedman's \acro{ANOVA} to be
somewhat underpowered relative to logistic regression. It highlighted
the practical consequences of the well-known problem of atypically-tuned commercial recordings. Although we would not propose deleting
outliers, it is sobering to know that tuning problems may be having an
outsized effect on our headline evaluation figures. It might be worth
considering allowing algorithms their best score in keys up to a
semitone above or below the ground truth.

Overall, we have shown that as \acro{ACE} becomes more established and
its evaluation becomes more thorough, it is useful to use a subtler
statistical model for comparative analysis. We recommend that future
\acro{MIREX} \acro{ACE} evaluations use logistic regression in
preference to Friedman's \acro{ANOVA}. It preserves the natural units
and scales of \acro{WCSR} and segementation analysis, is more
powerful for many (although not all) statistical tests, and crucially,
it allows for a more detailed correlational analysis of which
algorithms tend have problems with the same songs as others and which
have perhaps genuinely broken innovative ground. This is by no means
to suggest that Friedman's test is a bad test in general -- its
near-universal applicability makes it an excellent choice in many
circumstances, including many other \acro{MIREX} evaluations -- but
for \acro{ACE}, we believe that the extra understanding logistic regression can
offer may help researchers predict which techniques are most promising
for breaking the current performance plateau.

\bibliography{burgoyne2014comparative}

\end{document}
