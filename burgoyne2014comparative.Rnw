\documentclass{article}
\usepackage{ismir2014-jab,amsmath,cite}

\usepackage[utf8]{inputenc}
\usepackage[british]{babel}
\usepackage{csquotes}
\usepackage{url}
\usepackage{xcolor}

\usepackage{booktabs}
\usepackage{dcolumn}
\newcommand{\colhead}[1]{\multicolumn{1}{c}{#1}}
\usepackage[labelsep=period]{caption}
\usepackage{subcaption} 
% knitr requires a command for subfig compatibility
\newcommand{\subfloat}[2][need a sub-caption]{\subcaptionbox{#1}{#2}}
\usepackage{graphicx}

% for creating comments
\newcommand{\wrap}[3]{{\small \textcolor{#2}{[\textbf{#1:} #3]}}}
\newcommand{\Bas}[1]{\wrap{Bas}{blue}{#1}}
\newcommand{\Ashley}[1]{\wrap{Ashley}{red}{#1}}
\newcommand{\Johan}[1]{\wrap{Johan}{orange}{#1}}

\title{%
  On comparative statistics for labelling tasks\\%
  What can we learn from MIREX ACE $\mathbf{2013}$?%
}

\threeauthors
  {First author} {Affiliation$1$ \\ {\tt author1@ismir.edu}
    \thanks{Acknowledgements withheld to support blind review.}
}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation$3$ \\ {\tt author3@ismir.edu}}

% Post-review
% \threeauthors
%   {John Ashley Burgoyne} {Universiteit van Amsterdam \\ {\tt j.a.burgoyne@uva.nl}}
%   {Bas de Haas} {Universiteit Utrecht \\ {\tt w.b.dehaas@uu.nl}}
%   {Johan Pauwels} {{\tt johan.pauwels@gmail.com}}
  
\begin{document}
<<global-opts, cache=FALSE, include=FALSE>>=
library(knitr)  
library(ggplot2)   # figures
library(car)       # for contr.Sum()
library(geepack)   # geeglm() for GEEs
library(aod)       # vcov.geeglm(): needed for Tukey tests
library(multcomp)  # glht(), cld(), and mcp() for Tukey tests

opts_chunk$set(fig.align="center", cache=TRUE, echo=FALSE, warning=FALSE)

DATA.SETS <- c("MirexChord2009", 
               "BillboardTest2012", "BillboardTest2013")
## Algorithms in order of SeventhsBass performance.
ALGOS <- c("KO2", "NMSD2", "CB4", "NMSD1", "CB3", "KO1", 
           "PP4", "PP3", "CF2", "NG1", "NG2", "SB8")
VOCABS <- c("Root", 
            "MajMin", "MajMinBass", 
            "Sevenths", "SeventhsBass")
ANALYSES <- c(VOCABS, 
              "Segmentation")
DURATIONS <- c(sapply(tolower(VOCABS), 
                      function (x) paste0("Dur", x)),
               list(Segmentation = "DurRoot"))

FormatP <- function(p) {
  #' Format p-values according to APA specifications: 3 decimal places
  #' unless less than .001.
  #'
  ifelse(p < 0.001, "$p <$ .001", paste0("$p =$ .", formatC(1000 * p,
                                                            format = "f",
                                                            digits = 0,
                                                            width  = 3,
                                                            flag   = "0")))
}
@ 
%
\maketitle
%
\begin{abstract}
  For \acro{MIREX} 2013, the audio chord estimation \acro{(ACE)} task
  used a new evaluation scheme. Using chord vocabularies of differing
  complexity as well as segmentation measures, the new scheme provides
  more information than the \acro{ACE} evaluations from previous
  years. With this new information, however, comes new interpretive
  challenges. What are the correlations among different songs and,
  more importantly, different submissions across the new measures?
  Performance falls off for all submissions as the vocabularies
  increase in complexity, but does it do so directly in proportion to
  the number of more complex chords, or are certain algorithms indeed
  more robust? What are the outliers, song-algorithm pairs where the
  performance was substantially higher or lower than would be
  predicted, and how can they be explained? Answering these questions
  requires moving beyond the Friedman tests that have most often been
  used to compare algorithms to a richer underlying model. We propose
  a logistic-regression approach for generating comparative statistics
  for \acro{MIREX} \acro{ACE}, supported with generalised estimating
  equations \acro{(GEEs)} to correct for repeated measures. We use the
  \acro{MIREX} 2013 \acro{ACE} results as a case study to illustrate
  our proposed method, including some of interesting aspects of the
  evaluation that might not apparent from the headline results alone.
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

Automatic chord estimation \acro{(ACE)} has a long tradition within
the Music Information Retrieval \acro{(MIR)} community and
(automatically extracted) chords are generally recognised as a useful
harmonic representation in academia as well as in industry. For
instance, in an academic context it has been shown that chords are
interesting for addressing musicological
hypotheses~\cite{mauch2007,burgoynephd}, and that they can be used as
a mid-level feature to aid in retrieval tasks like cover-song
finding~\cite{haas2011,khadkevich2013}. In an industry setting, music
start-ups like Riffstation\footnote{\url{http://www.riffstation.com/}}
and Chordify\footnote{\url{http://chordify.net}}~\cite{haas2012} use
\acro{ACE} in their music teaching tools, and at time of writing
Chordify attracts more than 2 million unique visitors every month.

% 2008: 13 teams, 15 algorithms
% 2009: 10 teams, 18 algorithms
% 2010: 10 teams, 15 algorithms
% 2011:  9 teams, 18 algorithms
% 2012:  6 teams, 11 algorithms
% 2013:  7 teams, 12 algorithms

In order to compare different algorithmic approaches in an impartial setting, the \acro{MIREX} audio chord
estimation task has been established as a yearly event. Since
its inception in 2008, each year between 11 and 18 algorithms have been submitted 
by a number of teams ranging from 6 to 13. Despite the fact 
that \acro{ACE} algorithms are used outside of academic environments, 
and even though the number of \acro{MIREX} participants has decreased
slightly the last three years, the problem of automatic chord estimation is
nowhere near solved. On fresh validation data, the best-performing algorithms in
2013 barely reached a correct estimation in 75 percent of the time when considering only 24 major
and minor chords. This number quickly drops to maximally 60 percent when the 
evaluation is extended to a small subset of seventh chords.

Although \acro{MIREX} is a terrific platform for evaluating the performance of \acro{ACE}
algorithms, already in 2010 it was recognised that the evaluation of the
algorithms could be improved. At that time the quality of automatically
extracted chord sequences was evaluated by calculating the Chord Sequence Recall
\acro{(CSR)} that reflected the proportion of correctly labelled chords and a Weighted
Chord Sequence Recall \acro{(WCSR)} that weighted the grand average by the length of a
song. using a vocabulary of 12 major and minor chords. At \acro{ISMIR} 2010, a group of
ten researchers met to discuss their dissatisfaction with the \acro{ACE} evaluation
measures. In the resulting `Utrecht
Agreement'\footnote{\url{http://www.music-ir.org/mirex/wiki/The_Utrecht_Agreement_on_Chord_Evaluation}}
it was proposed that future evaluations should include more diverse chord
vocabularies, like seventh chords, inversions, etc., as the vocabulary of twelve
major and minor chords was considered a rather coarse representation of tonal
harmony. Furthermore, the necessity for an additional measure of segmentation 
of the generated chord sequences was agreed upon.

At approximately the same time, Christopher Harte proposed a formalisation of
measures that implemented the aspirations indicated in the Utrecht
agreement~\cite{harte2010}. \Bas{Maybe we should explain Harte's work in more
detail here.} Recently, Pauwels and Peeters~\cite{Pauwels-Peeters-2013}
reformulated and extended Harte's work with the precise aim to handle
differences in chord vocabulary between annotated ground truth and algorithmic
output on one hand, and in between different algorithms on the other hand. They
also performed a rigorous re-evaluation of all \acro{MIREX} \acro{ACE} submissions in
2010--2012. As of \acro{MIREX}
2013\footnote{\url{http://www.music-ir.org/mirex/wiki/2013:Audio_Chord_Estimation}},
these revised evaluation procedures, including the chord sequence segmentation
evaluation suggested by Harte~\cite{harte2010} and Mauch~\cite{mauch2010}, have
been adopted in the context of the \acro{MIREX} \acro{ACE} task.

\subsection{Objectives}

Limitations of Friedman test. Logistic regression as alternative. GEEs
to control for random effects.

Cite De Haas and Kutner for Friedman test.

\Bas{Somewhere at the end of this section I would appreciate a paragraph that summarises the main contribution of this paper.}

\section{Current Evaluation Battery for ACE}

<<read-results>>=
ReadResults <- function (analysis) {
  #' MIREX results for a particular chord vocabulary or segmentation
  #' 
  #' Assumes that the current directory is that of a particular database in
  #' Johan's distribution format.
  #'
  #' @param analysis chord vocabulary or segmentation to load
  #'
  is.segmentation <- analysis == "Segmentation"
  col.names <- c()
  col.classes <- c()
  if (is.segmentation) {
    setwd("resultsSegmentation")
    col.names <- c("song", "", "under.segmentation", "over.segmentation")
    col.classes <- c("factor", "NULL", "numeric", "numeric")
  } else {
    setwd(paste0("resultsMirex", analysis))
    col.names <- c("song", 
                   "performance", 
                   "duration",
                   rep("", 6))
    col.classes <- c("factor", "numeric", "numeric", rep("NULL", 6))
  }
  results <- data.frame()
  for (algo in ALGOS) {
    algo.results <- read.table(paste0(algo, ".csv"),
                               skip = 2,
                               header = FALSE,
                               sep = ",",
                               dec = ".",
                               col.names = col.names, 
                               colClasses = col.classes)
    if (is.segmentation) {
      ## Replace over- and under-segmentation with the 
      ## harmonic mean of their arithmetic inverses. 
      algo.results$performance <-
        1 / (0.5 * (1/(1-algo.results$under.segmentation) + 
                    1/(1-algo.results$over.segmentation)))
      algo.results$over.segmentation <- c()
      algo.results$under.segmentation <- c()
    } else {
      ## Rescale RCO results back to [0, 1].
      algo.results$performance <- algo.results$performance/100
    }
    results <- rbind(results, 
                     cbind(algorithm = rep(algo, dim(algo.results)[1]),
                           algo.results))
  }
  setwd("..")
  if (is.segmentation) results$duration <- ReadResults("Root")$duration
  results <-
    within(results, {
      performance.rank <- rep(NA, length(performance))
      for (group in levels(song))
        performance.rank[song==group] <- rank(performance[song==group])
    })
  return(results)
}

ReadAllResults <- function (data.set) {
  #' Complete battery of MIREX results
  #' 
  #' Assumes that the current directory contains directories of results
  #' for each database.
  #' 
  #' @param data.set the data set for which to load evaluations
  #'
  setwd(data.set)
  results <-
    within(
      Reduce(rbind,
             lapply(ANALYSES, 
                    function (analysis) {
                      result <- ReadResults(analysis)
                      result$analysis <- rep(analysis, dim(result)[1])
                      return(result)
                    }),
             data.frame()),
      {
        analysis <- factor(analysis, levels=ANALYSES)
        vocabulary <- analysis
        levels(vocabulary) <- c("Root", "MajMin", "MajMin", "Sevenths", "Sevenths", NA)
        bass <- analysis == "MajMinBass" | analysis == "SeventhsBass"
        bass[analysis == "Segmentation"] <- NA
        ## Order the algorithm factor for easier comparisons.
        algorithm <- factor(algorithm, levels=ALGOS)
        ## Set the contrasts such that they represent differences 
        ## from the grand average.
        contrasts(song) <- contr.Sum(levels(song))
        contrasts(algorithm) <- contr.Sum(levels(algorithm))
      })
  setwd("..")
  return(results[order(results$song),]) # group sorting for geeglm()
}

billboard.2013 <- ReadAllResults("BillboardTest2013")
@ 

\begin{table}
  \small
  \centering
 \begin{tabular}{lD{.}{.}{2.0}D{.}{.}{2.0}D{.}{.}{2.0}D{.}{.}{2.0}D{.}{.}{2.0}D{.}{.}{2.0}D{.}{.}{2.0}D{.}{}{2.0}}
    \toprule
    \colhead{Algo.} & \colhead{I} & \colhead{II} & \colhead{III} & \colhead{IV} &
    \colhead{V} & \colhead{VI} & \colhead{VII} & \colhead{VIII} \\
    \midrule
    \acro{KO2}$^\ast$   & 76 & 74 & 72 & 60 & 58 & 84 & 79 & 89 \\
    \acro{NMSD2} & 75 & 71 & 69 & 59 & 57 & 82 & 79 & 86 \\
    \acro{CB4}$^\ast$   & 76 & 72 & 70 & 59 & 57 & 85 & 80 & 90 \\
    \acro{NMSD1} & 74 & 71 & 69 & 58 & 56 & 83 & 79 & 86 \\
    \acro{CB3}   & 76 & 72 & 70 & 58 & 56 & 85 & 81 & 89 \\
    \acro{KO1}   & 75 & 71 & 69 & 54 & 52 & 83 & 80 & 88 \\
    \acro{PP4}   & 69 & 66 & 64 & 51 & 49 & 83 & 78 & 87 \\
    \acro{PP3}   & 70 & 68 & 65 & 50 & 48 & 83 & 82 & 84 \\
    \acro{CF2}   & 71 & 67 & 65 & 49 & 47 & 83 & 83 & 83 \\
    \acro{NG1}   & 71 & 67 & 65 & 49 & 46 & 82 & 79 & 86 \\
    \acro{NG2}   & 67 & 63 & 61 & 44 & 43 & 82 & 81 & 83 \\
    \acro{SB8}   &  9 &  7 &  6 &  5 &  5 & 51 & 92 & 35 \\
    \bottomrule
  \end{tabular}
  \caption{Results for \acro{MIREX} \acro{ACE} 2013 on the
    \emph{Billboard} 2013 test set. I: root only; II: major-minor
    vocabulary; III: major-minor vocabulary with inversions; IV:
    major-minor vocabulary with sevenths; V: major-minor vocabulary
    with sevenths and inversions; VI: mean segmentation score; VII:
    under-segmentation; VIII: over-segmentation. Starred algorithms
    incorporated local training on the \acro{MIREX} servers. Adapted
    from \protect\url{http://music-ir.org/mirex/wiki/2013:Audio_Chord_Estimation_Results_Billboard_2013}.}
  \label{tab:mirex-results}
\end{table}

<<cld-plots, dependson="read-results", fig.show="hold", fig.width=12, fig.height=6, include=FALSE>>=
friedman.summary <-
  with(droplevels(billboard.2013[billboard.2013$analysis=="SeventhsBass",]),
       summary(glht(aov(performance.rank ~ song + algorithm),
                    mcp(algorithm = "Tukey")),
               test = adjusted("fdr")))
logistic.summary <-
  with(droplevels(billboard.2013[billboard.2013$analysis=="SeventhsBass",]),
       summary(glht(geeglm(formula = performance ~ algorithm,
                           family  = binomial(link="logit"),
                           corstr  = "exchangeable",
                           weights = duration,
                           id      = song),
                    mcp(algorithm = "Tukey")),
               test = adjusted("fdr")))
## Compact letter displays require an extra top margin.
par(mar = par("mar") + c(0, 0, 5, 0))
## Trick plot.cld into using the response for the Friedman ANOVA.
friedman.cld <- cld(friedman.summary, level=.005)
friedman.cld$covar <- FALSE
plot(friedman.cld,
     xlab = "Algorithm",
     ylab = "Rank per Song (1 low; 12 high)")
plot(cld(logistic.summary, level=.005),
     xlab = "Algorithm",
     ylab = "Weighted Chord-Symbol Recall")
@ 

\begin{figure*}
  \centering
  \subfloat[Friedman's \acro{ANOVA}\label{fig:cld-plots1}]{
    \centering
    \includegraphics[width=\linewidth]{figure/cld-plots1} 
  }\\
  \subfloat[Logistic Regression\label{fig:cld-plots2}]{
    \centering
    \includegraphics[width=\linewidth]{figure/cld-plots2} 
  }
  \caption{Boxplots and compact letter displays for the \acro{MIREX}
    \acro{ACE} 2013 results on the \emph{Billboard} 2013 test set with
    vocabulary V (seventh chords and inversions). Given the
    statistical models, there are insufficient data to distinguish
    among algorithms sharing a letter, correcting for multiple
    comparisons to hold the false discovery rate at $\alpha <$
    .005. $N =$ 161 songs per algorithm. Although Friedman's
    \acro{ANOVA} detects slightly more significant pairwise
    differences than the logistic regression model
    (\Sexpr{sum(friedman.summary$test$pvalues < .005)} vs.\
    \Sexpr{sum(logistic.summary$test$pvalues < .005)}), it operates on
    a different scale than \acro{CSR} and misorders algorithms
    relative to the headline results.}
  \label{fig:clr-plots}
\end{figure*}

Isophonics and two Billboard sets. Limit ourselves to Billboard
2013. Sampling procedure and sample size.

Research design in general: i.e., all algorithms on all
songs. Measures and covariates: WCSR and segmentation; different vocabularies.

\section{Logistic Regression with GEEs}

\section{Results}

\subsection{Tukey tests}

Vocabulary or vocabularies and segmentation.

\subsection{Effect of vocabulary}

<<vocabulary, dependson="read-results">>=
vocab.fit. <-
  with(droplevels(billboard.2013[billboard.2013$analysis != "Segmentation",]),
       geeglm(performance ~ algorithm * analysis, 
                family = binomial(link="logit"),
                corstr = "exchangeable",
                weights = duration,
                id = song)
       )
vocab.fit.2 <-
  with(droplevels(billboard.2013[billboard.2013$analysis != "Segmentation",]),
         geeglm(performance ~ algorithm * (vocabulary + bass), 
                family = binomial(link="logit"),
                corstr = "exchangeable",
                weights = duration,
                id = song)
       )
vocab.fit.3 <-
  with(droplevels(billboard.2013[billboard.2013$analysis != "Segmentation",]),
         geeglm(performance ~ algorithm + vocabulary + bass, 
                family = binomial(link="logit"),
                corstr = "exchangeable",
                weights = duration,
                id = song)
    )
vocab.fit.4 <-
  with(droplevels(billboard.2013[billboard.2013$analysis != "Segmentation",]),
         geeglm(performance ~ algorithm,
                family = binomial(link="logit"),
                corstr = "exchangeable",
                weights = duration,
                id = song)
    )
@ 

\subsection{Correlation matrices}

\subsection{Outliers}

\section{Discussion}

\subsection{Utility of new vocabularies}

\subsection{Comparison with Friedman test}

Comparative figure.

\subsection{Correlation matrices}

Include MDS figure(s).

\subsection{Outliers}

\subsection{Interpretation and generalisability}

\subsection{Comparison with Pauwels \& Peeters}

\section{Implications for Future Work}

\bibliography{burgoyne2014comparative}

\end{document}
