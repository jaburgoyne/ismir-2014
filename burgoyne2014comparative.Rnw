\documentclass{article}
\usepackage{ismir2014-jab,amsmath,cite}

\usepackage[utf8]{inputenc}
\usepackage[british]{babel}
\usepackage{csquotes}
\usepackage{url}
\usepackage{xcolor}

\usepackage{graphicx}

% for creating comments
\newcommand{\wrap}[3]{{\small \textcolor{#2}{[\textbf{#1:} #3]}}}
\newcommand{\Bas}[1]{\wrap{Bas}{blue}{#1}}
\newcommand{\Ashley}[1]{\wrap{Ashley}{red}{#1}}
\newcommand{\Johan}[1]{\wrap{Johan}{orange}{#1}}

\title{%
  On comparative statistics for labelling tasks\\%
  What can we learn from MIREX ACE $\mathbf{2013}$?%
}

\threeauthors
  {First author} {Affiliation$1$ \\ {\tt author1@ismir.edu}
    \thanks{Acknowledgements withheld to support blind review.}
}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation$3$ \\ {\tt author3@ismir.edu}}

% Post-review
% \threeauthors
%   {John Ashley Burgoyne} {Universiteit van Amsterdam \\ {\tt j.a.burgoyne@uva.nl}}
%   {Bas de Haas} {Universiteit Utrecht \\ {\tt w.b.dehaas@uu.nl}}
%   {Johan Pauwels} {{\tt johan.pauwels@gmail.com}}
  
\begin{document}
%
\maketitle
%
\begin{abstract}
  For \acro{MIREX} 2013, the audio chord estimation \acro{(ACE)} task
  used a new evaluation scheme. Using chord vocabularies of differing
  complexity as well as segmentation measures, the new scheme provides
  more information than the \acro{ACE} evaluations from previous
  years. With this new information, however, comes new interpretive
  challenges. What are the correlations among different songs and,
  more importantly, different submissions across the new measures?
  Performance falls off for all submissions as the vocabularies
  increase in complexity, but does it do so directly in proportion to
  the number of more complex chords, or are certain algorithms indeed
  more robust? What are the outliers, song-algorithm pairs where the
  performance was substantially higher or lower than would be
  predicted, and how can they be explained? Answering these questions
  requires moving beyond the Friedman tests that have most often been
  used to compare algorithms to a richer underlying model. We propose
  a logistic-regression approach for generating comparative statistics
  for \acro{MIREX} \acro{ACE}, supported with generalised estimating
  equations \acro{(GEEs)} to correct for repeated measures. We use the
  \acro{MIREX} 2013 \acro{ACE} results as a case study to illustrate
  our proposed method, including some of interesting aspects of the
  evaluation that might not apparent from the headline results alone.
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

Automatic chord estimation \acro{(ACE)} has a long tradition within
the Music Information Retrieval \acro{(MIR)} community and
(automatically extracted) chords are generally recognised as a useful
harmonic representation in academia as well as in industry. For
instance, in an academic context it has been shown that chords are
interesting for addressing musicological
hypotheses~\cite{mauch2007,burgoynephd}, and that they can be used as
a mid-level feature to aid in retrieval tasks like cover-song
finding~\cite{haas2011,khadkevich2013}. In an industry setting, music
start-ups like Riffstation\footnote{\url{http://www.riffstation.com/}}
and Chordify\footnote{\url{http://chordify.net}}~\cite{haas2012} use
\acro{ACE} in their music teaching tools, and at time of writing
Chordify attracts more than 2 million unique visitors every month.

% 2008: 13 teams, 15 algorithms
% 2009: 10 teams, 18 algorithms
% 2010: 10 teams, 15 algorithms
% 2011:  9 teams, 18 algorithms
% 2012:  6 teams, 11 algorithms
% 2013:  7 teams, 12 algorithms

In order to compare different algorithmic approaches in an impartial setting, the \acro{MIREX} audio chord
estimation task has been established as a yearly event. Since
its inception in 2008, each year between 11 and 18 algorithms have been submitted 
by a number of teams ranging from 6 to 13. Despite the fact 
that \acro{ACE} algorithms are used outside of academic environments, 
and even though the number of \acro{MIREX} participants has decreased
slightly the last three years, the problem of automatic chord estimation is
nowhere near solved. On fresh validation data, the best-performing algorithms in
2013 barely reached a correct estimation in 75 percent of the time when considering only 24 major
and minor chords. This number quickly drops to maximally 60 percent when the 
evaluation is extended to a small subset of seventh chords.

Although \acro{MIREX} is a terrific platform for evaluating the performance of \acro{ACE}
algorithms, already in 2010 it was recognised that the evaluation of the
algorithms could be improved. At that time the quality of automatically
extracted chord sequences was evaluated by calculating the Chord Sequence Recall
\acro{(CSR)} that reflected the proportion of correctly labelled chords and a Weighted
Chord Sequence Recall \acro{(WCSR)} that weighted the grand average by the length of a
song. using a vocabulary of 12 major and minor chords. At \acro{ISMIR} 2010, a group of
ten researchers met to discuss their dissatisfaction with the \acro{ACE} evaluation
measures. In the resulting `Utrecht
Agreement'\footnote{\url{http://www.music-ir.org/mirex/wiki/The_Utrecht_Agreement_on_Chord_Evaluation}}
it was proposed that future evaluations should include more diverse chord
vocabularies, like seventh chords, inversions, etc., as the vocabulary of twelve
major and minor chords was considered a rather coarse representation of tonal
harmony. Furthermore, the necessity for an additional measure of segmentation 
of the generated chord sequences was agreed upon.

At approximately the same time, Christopher Harte proposed a formalisation of
measures that implemented the aspirations indicated in the Utrecht
agreement~\cite{harte2010}. \Bas{Maybe we should explain Harte's work in more
detail here.} Recently, Pauwels and Peeters~\cite{Pauwels-Peeters-2013}
reformulated and extended Harte's work with the precise aim to handle
differences in chord vocabulary between annotated ground truth and algorithmic
output on one hand, and in between different algorithms on the other hand. They
also performed a rigorous re-evaluation of all \acro{MIREX} \acro{ACE} submissions in
2010--2012. As of \acro{MIREX}
2013\footnote{\url{http://www.music-ir.org/mirex/wiki/2013:Audio_Chord_Estimation}},
these revised evaluation procedures, including the chord sequence segmentation
evaluation suggested by Harte~\cite{harte2010} and Mauch~\cite{mauch2010}, have
been adopted in the context of the \acro{MIREX} \acro{ACE} task.

\subsection{Objectives}

Limitations of Friedman test. Logistic regression as alternative. GEEs
to control for random effects.

\Bas{Somewhere at the end of this section I would appreciate a paragraph that summarises the main contribution of this paper.}

\section{Current Evaluation Battery for ACE}

Isophonics and two Billboard sets. Limit ourselves to Billboard
2013. Sampling procedure and sample size.

Research design in general: i.e., all algorithms on all
songs. Measures and covariates: WCSR and segmentation; different vocabularies.

\section{Logistic Regression with GEEs}

\section{Results}

\subsection{Effect of vocabulary}

\subsection{Tukey tests}

Vocabulary or vocabularies and segmentation.

\subsection{Correlation matrices}

\subsection{Outliers}

\section{Discussion}

\subsection{Utility of new vocabularies}

\subsection{Comparison with Friedman test}

Comparative figure.

\subsection{Correlation matrices}

Include MDS figure(s).

\subsection{Outliers}

\subsection{Interpretation and generalisability}

\subsection{Comparison with Pauwels \& Peeters}

\section{Implications for Future Work}

\bibliography{burgoyne2014comparative}

\end{document}
