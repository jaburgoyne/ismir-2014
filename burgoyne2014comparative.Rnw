\documentclass{article}
\usepackage{ismir2014,amsmath,cite}

\usepackage[utf8]{inputenc}
\usepackage[british]{babel}
\usepackage{csquotes}
\usepackage{url}

% Bas: this packages won't compile on my machine
%\usepackage[osf]{newtxtext}
\usepackage{newtxmath}
% The TeX-Live newtx package is out of date, so correct math figures.
\newcommand*{\mathdefault}{ntxrx}
\DeclareSymbolFont{operators}{\encodingdefault}{\mathdefault}{m}{n}
\SetSymbolFont{operators}{bold}{\encodingdefault}{\mathdefault}{b}{n}
\DeclareMathAlphabet{\mathit}{\encodingdefault}{\mathdefault}{m}{it}
\DeclareMathAlphabet{\mathbf}{\encodingdefault}{\mathdefault}{b}{n}
\SetMathAlphabet{\mathit}{bold}{\encodingdefault}{\mathdefault}{b}{it}
\usepackage[tracking,letterspace=50,babel]{microtype}
\newcommand*{\acro}[1]{\textsc{\MakeLowercase{#1}}}
\usepackage{titlesec}
\titleformat{\section}{\fontfamily{ntxrx}\bfseries\selectfont\centering\uppercase}{\thesection.}{0.6em}{}
\titleformat{\subsection}{\fontfamily{ntxrx}\bfseries\selectfont\raggedright}{\thesubsection}{0.6em}{}

\usepackage{graphicx}

% for creating comments
\newcommand{\wrap}[3]{{\small \textcolor{#2}{[\textbf{#1:} #3]}}}
\newcommand{\Bas}[1]{\wrap{Bas}{blue}{#1}}
\newcommand{\Ashley}[1]{\wrap{Ashley}{red}{#1}}
\newcommand{\Johan}[1]{\wrap{Johan}{orange}{#1}}


\title{%
  On comparative statistics for labelling tasks\\%
  What can we learn from MIREX ACE $\mathbf{2013}$?%
}

\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Post-review
% \threeauthors
%   {John Ashley Burgoyne} {Universiteit van Amsterdam \\ {\tt j.a.burgoyne@uva.nl}}
%   {Bas de Haas} {Universiteit Utrecht \\ {\tt w.b.dehaas@uu.nl}}
%   {Johan Pauwels} {{\tt johan.pauwels@gmail.com}}

\begin{document}
%
\maketitle
%
\begin{abstract}
  For \acro{MIREX} 2013, the audio chord estimation \acro{(ACE)} task
  used a new evaluation scheme. Using chord vocabularies of differing
  complexity as well as segmentation measures, the new scheme provides
  more information than the \acro{ACE} evaluations from previous
  years. With this new information, however, comes new interpretive
  challenges. What are the correlations among different songs and,
  more importantly, different submissions across the new measures?
  Performance falls off for all submissions as the vocabularies
  increase in complexity, but does it do so directly in proportion to
  the number of more complex chords, or are certain algorithms indeed
  more robust? What are the outliers, song-algorithm pairs where the
  performance was substantially higher or lower than would be
  predicted, and how can they be explained? Answering these questions
  requires moving beyond the Friedman tests that have most often been
  used to compare algorithms to a richer underlying model. We propose
  a logistic-regression approach for generating comparative statistics
  for \acro{MIREX ACE}, supported with generalised estimating
  equations \acro{(GEEs)} to correct for repeated measures. We use the
  \acro{MIREX} 2013 \acro{ACE} results as a case study to illustrate
  our proposed method, including some of interesting aspects of the
  evaluation that might not apparent from the headline results alone.
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

\Bas{Will we use US or UK English?}

\Bas{Introduce the acronym ACE somewhere in the introduction}

\Bas{Somewhere at the end of this section I would appreciate a paragraph that summarises the main contribution of this paper.}

\subsection{History of ACE Evaluation}

Automatic chord recognition has a long tradition within the Music Information
Retrieval (MIR) community and (automatically extracted) chords are generally
recognised as a useful harmonic representation in academia as well as in
industry. For instance, in an academic context it has been shown that chords are
interesting for addressing musicological
hypotheses~\cite{mauch2007,burgoynephd}, and that they can be used as a
mid-level feature to aid in retrieval tasks like cover-song
finding~\cite{haas2011,khadkevich2013}. In an industry setting, music start-ups
like Chordify\footnote{\url{http://chordify.net}}~\cite{haas2012} apply ACE to
various on-line music sources, and at time of writing Chordify is attracts for more
than 2 million unique visitors every month. \Johan{http://www.riffstation.com, an ACE supported guitar practice app from Mikel Gainza and others from Dublin Institute of Technology can be used as another industry example}

% 2008: 13 teams, 15 algorithms
% 2009: 10 teams, 18 algorithms
% 2010: 10 teams, 15 algorithms
% 2011:  9 teams, 18 algorithms
% 2012:  6 teams, 11 algorithms
% 2013:  7 teams, 12 algorithms

In order to compare different algorithmic approaches in an impartial setting, the MIREX audio chord
estimation task has been established as a yearly event. Since
its inception in 2008, each year between 11 and 18 algorithms have been submitted by a number of teams ranging from 6 to 13. Despite the fact that ACE algorithms are already used outside of academic environments, and even though the number of MIREX participants has decreased
slightly the last three years, the problem of automatic chord transcription \Johan{wouldn't it be clearer if we stuck to a single designation of the task: estimation/extraction/transcription, pick one} is
nowhere near solved. On fresh validation data, the best-performing algorithms in
2013 barely reached a correct estimation in 75 percent of the time when considering only 24 major
and minor chords. This number quickly drops off to 60~\% or less when the evaluation is extended to just a small subset of seventh chords.

Although MIREX is a terrific platform for evaluating the performance of ACE
algorithms, already in 2010 it was recognised that the evaluation of the
algorithms could be improved. At that time the quality of automatically
extracted chord sequences was evaluated by calculating the Chord Sequence Recall
(CSR) that reflected the proportion of correctly labelled chords and a Weighted
Chord Sequence Recall (WCSR) that weighted the grand average by the length of a
song. using a vocabulary of 12 major and minor chords. At ISMIR 2010, a group of
ten researchers met to discuss their dissatisfaction with the ACE evaluation
measures. In the resulting `Utrecht
Agreement'\footnote{\url{http://www.music-ir.org/mirex/wiki/The_Utrecht_Agreement_on_Chord_Evaluation}}
it was proposed that future evaluations should include more diverse chord
vocabularies, like seventh chords, inversions, etc., as the vocabulary of twelve
major and minor chords was considered a rather coarse representation of tonal
harmony. Furthermore, the necessity for an additional measure of segmentation of the generated chord sequences was agreed on.

At approximately the same time, Christopher Harte proposed a formalisation of measures that implemented the aspirations
indicated in the Utrecht agreement~\cite{harte2010}. \Bas{Maybe we should
explain Harte's work in more detail here.} Recently, Pauwels and
Peeters~\cite{Pauwels-Peeters-2013} reformulated and extended Harte's work with the precise aim to handle differences in chord vocabulary between annotated ground truth and algorithmic output on one hand, and in between different algorithms on the other hand. They also performed a rigorous re-evaluation of all
MIREX ACE submissions in 2010--2012. As of MIREX
2013\footnote{\url{http://www.music-ir.org/mirex/wiki/2013:Audio_Chord_Estimation}}, these revised evaluation procedures, including the chord sequence segmentation evaluation suggested by Harte~\cite{harte2010} and Mauch~\cite{mauch2010}, have been adopted in the context of the MIREX ACE task.


\section{Logistic Regression with GEEs}

%\bibliographystyle{ismir2014}
\bibliography{burgoyne2014comparative}

\end{document}
