\documentclass[a4paper]{article}
\usepackage{ismir2014-jab,amsmath,cite}

\usepackage[utf8]{inputenc}
\usepackage[british]{babel}
\usepackage{csquotes}
\usepackage{url}
\usepackage{xcolor}

\usepackage{vector}
\renewcommand*{\vec}[1]{\ensuremath{\boldsymbol{\bvec{#1}}}}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\usepackage{xfrac}

\usepackage{booktabs}
\usepackage{dcolumn}
\newcommand{\colhead}[1]{\multicolumn{1}{c}{#1}}
\usepackage[labelsep=period]{caption}
\usepackage{subcaption} 
% knitr requires a command for subfig compatibility
\newcommand{\subfloat}[2][need a sub-caption]{\subcaptionbox{#1}{#2}}
\usepackage{graphicx}

% for creating comments
\newcommand{\wrap}[3]{{\small \textcolor{#2}{[\textbf{#1:} #3]}}}
\newcommand{\Bas}[1]{\wrap{Bas}{blue}{#1}}
\newcommand{\Ashley}[1]{\wrap{Ashley}{red}{#1}}
\newcommand{\Johan}[1]{\wrap{Johan}{orange}{#1}}

\title{%
  On comparative statistics for labelling tasks:\\%
  What can we learn from MIREX ACE $\mathbf{2013}$?%
}

\threeauthors
  {First author} {Affiliation$1$ \\ {\tt author1@ismir.edu}
    \thanks{Acknowledgements withheld to support blind review.}
}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation$3$ \\ {\tt author3@ismir.edu}}

% Post-review
% \threeauthors
%   {John Ashley Burgoyne} {Universiteit van Amsterdam \\ {\tt j.a.burgoyne@uva.nl}}
%   {Bas de Haas} {Universiteit Utrecht \\ {\tt w.b.dehaas@uu.nl}}
%   {Johan Pauwels} {{\tt johan.pauwels@gmail.com}}
  
\begin{document}
<<global-opts, cache=FALSE, include=FALSE>>=
library(knitr)  
library(ggplot2)   # figures
library(car)       # for contr.Sum()
library(geepack)   # geeglm() for GEEs
library(aod)       # vcov.geeglm(): needed for Tukey tests
library(multcomp)  # glht(), cld(), and mcp() for Tukey tests

opts_chunk$set(fig.align="center", cache=TRUE, echo=FALSE, warning=FALSE)

DATA.SETS <- c("MirexChord2009", 
               "BillboardTest2012", "BillboardTest2013")
## Algorithms in order of SeventhsBass performance.
ALGOS <- c("KO2", "NMSD2", "CB4", "NMSD1", "CB3", "KO1", 
           "PP4", "PP3", "CF2", "NG1", "NG2", "SB8")
VOCABS <- c("Root", 
            "MajMin", "MajMinBass", 
            "Sevenths", "SeventhsBass")
ANALYSES <- c(VOCABS, 
              "Segmentation")
DURATIONS <- c(sapply(tolower(VOCABS), 
                      function (x) paste0("Dur", x)),
               list(Segmentation = "DurRoot"))

FormatP <- function(p) {
  #' Format p-values according to APA specifications: 3 decimal places
  #' unless less than .001.
  #'
  ifelse(p < 0.001, "$p <$ .001", paste0("$p =$ .", formatC(1000 * p,
                                                            format = "f",
                                                            digits = 0,
                                                            width  = 3,
                                                            flag   = "0")))
}
@ 
%
\maketitle
%
\begin{abstract}
  For \acro{MIREX} 2013, the audio chord estimation \acro{(ACE)} task
  used a new evaluation scheme. Using chord vocabularies of differing
  complexity as well as segmentation measures, the new scheme provides
  more information than the \acro{ACE} evaluations from previous
  years. With this new information, however, comes new interpretive
  challenges. What are the correlations among different songs and,
  more importantly, different submissions across the new measures?
  Performance falls off for all submissions as the vocabularies
  increase in complexity, but does it do so directly in proportion to
  the number of more complex chords, or are certain algorithms indeed
  more robust? What are the outliers, song-algorithm pairs where the
  performance was substantially higher or lower than would be
  predicted, and how can they be explained? Answering these questions
  requires moving beyond the Friedman tests that have most often been
  used to compare algorithms to a richer underlying model. We propose
  a logistic-regression approach for generating comparative statistics
  for \acro{MIREX} \acro{ACE}, supported with generalised estimating
  equations \acro{(GEEs)} to correct for repeated measures. We use the
  \acro{MIREX} 2013 \acro{ACE} results as a case study to illustrate
  our proposed method, including some of interesting aspects of the
  evaluation that might not apparent from the headline results alone.
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

Automatic chord estimation \acro{(ACE)} has a long tradition within
the music information retrieval \acro{(MIR)} community, and
(automatically extracted) chords are generally recognised as a useful
mid-level representation in academia as well as in industry. For
instance, in an academic context it has been shown that chords are
interesting for addressing musicological
hypotheses~\cite{mauch2007,burgoynephd}, and that they can be used as
a mid-level feature to aid in retrieval tasks like cover-song
detection~\cite{haas2011,khadkevich2013}. In an industry setting,
music start-ups like
Riffstation\footnote{\url{http://www.riffstation.com/}} and
Chordify\footnote{\url{http://chordify.net}} use \acro{ACE} in their
music teaching tools, and at time of writing Chordify attracts more
than 2 million unique visitors every month~\cite{haas2012}.

% 2008: 13 teams, 15 algorithms
% 2009: 10 teams, 18 algorithms
% 2010: 10 teams, 15 algorithms
% 2011:  9 teams, 18 algorithms
% 2012:  6 teams, 11 algorithms
% 2013:  7 teams, 12 algorithms

In order to compare different algorithmic approaches in an impartial
setting, the Music Information Retrieval Evaluation eXchange
\acro{(MIREX)} audio chord estimation task was established as an
annual event. Since its inception in 2008, between 11 and 18
algorithms have been submitted each year by between 6 and 13
teams. Despite the fact that \acro{ACE} algorithms are used outside of
academic environments, and even though the number of \acro{MIREX}
participants has decreased slightly the last three years, the problem
of automatic chord estimation is nowhere near solved. On fresh
validation data, the best-performing algorithms in 2013 estimated
chords correctly only 75 percent of the time, and that only when the
range of possible chords was restricted exclusively to the 24 major
and minor chords; the performance figure drops to 60 percent when the
evaluation is extended to include seventh chords (see Table~\ref{tab:mirex-results}).

\begin{table*}
  \small
  \centering
 \begin{tabular}{clcD{.}{}{2.0}ccD{.}{}{2.0}D{.}{}{2.0}D{.}{}{2.0}D{.}{}{2.0}D{.}{}{2.0}D{.}{}{2.0}D{.}{}{2.0}D{.}{}{2.0}}
    \toprule
    \multicolumn{3}{c}{Algorithm} &
    \colhead{\# Types} & \colhead{Inversions?} & \colhead{Training?} &
    \colhead{I} & \colhead{II} & \colhead{III} & \colhead{IV} &
    \colhead{V} & \colhead{VI} & \colhead{VII} & \colhead{VIII} \\
    \midrule
    & \acro{KO2}   & &  7 & & \textbullet & 76 & 74 & 72 & 60 & 58 & 84 & 79 & 89 \\
    & \acro{NMSD2} & & 10 & &             & 75 & 71 & 69 & 59 & 57 & 82 & 79 & 86 \\
    & \acro{CB4}   & & 13 & & \textbullet & 76 & 72 & 70 & 59 & 57 & 85 & 80 & 90 \\
    & \acro{NMSD1} & & 10 & &             & 74 & 71 & 69 & 58 & 56 & 83 & 79 & 86 \\
    & \acro{CB3}   & & 13 & &             & 76 & 72 & 70 & 58 & 56 & 85 & 81 & 89 \\
    & \acro{KO1}   & &  7 & &             & 75 & 71 & 69 & 54 & 52 & 83 & 80 & 88 \\
    & \acro{PP4}   & &  5 & &             & 69 & 66 & 64 & 51 & 49 & 83 & 78 & 87 \\
    & \acro{PP3}   & &  2 & &             & 70 & 68 & 65 & 50 & 48 & 83 & 82 & 84 \\
    & \acro{CF2}   & & 10 & \textbullet & & 71 & 67 & 65 & 49 & 47 & 83 & 83 & 83 \\
    & \acro{NG1}   & &  2 & &             & 71 & 67 & 65 & 49 & 46 & 82 & 79 & 86 \\
    & \acro{NG2}   & &  5 & &             & 67 & 63 & 61 & 44 & 43 & 82 & 81 & 83 \\
    & \acro{SB8}   & &  2 & &             &  9 &  7 &  6 &  5 &  5 & 51 & 92 & 35 \\
    \bottomrule
  \end{tabular}
  \caption{Number of supported chord types, inversion support,
    training support, and \acro{MIREX} results on the
    \emph{Billboard} 2013 test set for all 2013 \acro{ACE}
    submissions. I: root only; II: major-minor
    vocabulary; III: major-minor vocabulary with inversions; IV:
    major-minor vocabulary with sevenths; V: major-minor vocabulary
    with sevenths and inversions; VI: mean segmentation score; VII:
    under-segmentation; VIII: over-segmentation. Adapted from the
    \acro{MIREX} Wiki.}
  \label{tab:mirex-results}
\end{table*}

\acro{MIREX} is a terrific platform for evaluating the performance of
\acro{ACE} algorithms, but by 2010 it was already being recognised
that the evaluation rubrics could be improved. At that time, the
quality of automatically extracted chord sequences was evaluated by
calculating the \emph{chord symbol recall} \acro{(CSR)}, which
reflected the proportion of correctly labelled chords, and a
\emph{weighted chord symbol recall} \acro{(WCSR)}, which weighted the
grand average by the length of a song, using a vocabulary of 12 major
and minor chords. At \acro{ISMIR} 2010, a group of ten researchers met
to discuss their dissatisfaction with the \acro{ACE} evaluation
measures. In the resulting \enquote{Utrecht Agreement}, it was
proposed that future evaluations should include more diverse chord
vocabularies, such as seventh chords and inversions, as the 24-chord
vocabulary was considered a rather coarse representation of tonal
harmony.\footnote{\url{http://www.music-ir.org/mirex/wiki/The_Utrecht_Agreement_on_Chord_Evaluation}}
Furthermore, the group agreed that it was important to include a
measure of segmentation quality.

At approximately the same time, Christopher Harte proposed a
formalisation of measures that implemented the aspirations indicated
in the Utrecht agreement~\cite{harte2010}. Recently, Pauwels and
Peeters reformulated and extended Harte's work with the precise aim of
handling differences in chord vocabulary between annotated ground
truth and algorithmic output on one hand, and among the output of
different algorithms on the other
hand~\cite{Pauwels-Peeters-2013}. They also performed a rigorous
re-evaluation of all \acro{MIREX} \acro{ACE} submissions from 2010 to
2012. As of \acro{MIREX} 2013, these revised evaluation procedures,
including the chord sequence segmentation evaluation suggested by
Harte~\cite{harte2010} and Mauch~\cite{mauch2010}, have been adopted
in the context of the \acro{MIREX} \acro{ACE}
task.\footnote{\url{http://www.music-ir.org/mirex/wiki/2013:Audio_Chord_Estimation}}

It is difficult to discern a story in the spray of numbers in
Table~\ref{tab:mirex-results}, and thus \acro{MIREX} evaluation has
also typically included comparative statistics to help determine
whether the difference in performance between pairs of algorithms is
statistically significant. Traditionally, Friedman's \acro{ANOVA} has
been used for this purpose, accompanied by Tukey's Honest Significant
Difference tests for each pair of algorithms. Friedman's \acro{ANOVA}
is equivalent to a standard two-way \acro{ANOVA} with the actual
measurements (in our case \acro{WCSR} or the new segmentation measure)
replaced by the rank of each treatment (in our case, each algorithm)
on that measure within each block (in our case, for each song)
\cite{Kutner2005}. The rank transformation makes
Friedman's \acro{ANOVA} an excellent \enquote{one size fits all}
approach that can be applied with minimal regard to the underlying
distribution of the data, but these benefits come with costs. Like any
non-parametric test, Friedman's \acro{ANOVA} can be less powerful than
parametric alternatives where the distribution is known, and the rank
transformation can obscure information inherent to the underlying
measurement, magnifying trivial differences and neutralising
significant inter-correlations.
\Bas{There is another drawback of the Friedman test, it cannot naturally
handle weighted evauation scores, such as the \arc{WCSR}. 
In the particular case of chord labelling the experimenter would
prefer the \acro{WCSR} over the \acro{CSR}~\cite{Pauwels-Peeters-2013},
but he can only use the \acro{CSR} for statistical anlalysis using the
Friedman test.}

There is no need to pay the costs of Friedman's \acro{ANOVA} for
evaluating chord estimation, however. Fundamentally, \acro{WCSR} is a
proportion, specifically the expected proportion of audio frames that
an estimation algorithm will label correctly, and as such, it fits
naturally into \emph{logistic regression} (i.e., a \emph{logit
  model}). Likewise, the new segmentation score is constrained to fall
between 0 and 100 percent, and thus it is also suitable for the same
type of analysis. The remainder of this paper describes how logistic
regression can be used to compare chord estimation algorithms, using
\acro{MIREX} results from last year to illustrate four key benefits:
easier interpretation, greater statistical power, an inter-correlation
for identify relationships among submitted algorithms, and better
detection of outliers.

\section{Logistic Regression with GEEs}

Proportions cannot be distributed normally because they are supported
exclusively on [0, 1], and thus they present challenges for
traditional techniques of statistical analysis. Logit models are
designed to handle these challenges without sacrificing the simplicity
of the usual linear function relating parameters and covariates
\cite[ch.~4]{Agresti2007}:
\begin{equation}
  \label{eq:logit1}
  \pi(\vec{x}; \vec{\betaup}) = 
  \frac{\mathrm{e}^{\vec{x}'\vec{\betaup}}}
  {1 + \mathrm{e}^{\vec{x}'\vec{\betaup}}}
  \ ,
\end{equation}
or equivalently
\begin{equation}
  \label{eq:logit2}
  \log \frac{\pi(\vec{x}; \vec{\betaup})}{1-\pi(\vec{x}; \vec{\betaup})} = \vec{x}'\vec{\betaup}
  \ ,
\end{equation}
where $\pi(\vec{x}; \vec{\betaup})$ represents the relative frequency
of \enquote{success} given the values of covariates in $\vec{x}$ and
parameters $\vec{\betaup}$. In the case of a basic model for
\acro{MIREX} \acro{ACE}, $\vec{x}$ would identify the algorithm and
$\pi(\vec{x}; \vec{\betaup})$ would be the relative frequency of
correct chord labels for that algorithm (i.e., \acro{WCSR}). In the
case of data like \acro{ACE} results, where there are proportions
$p_i$ of correct recognitions over $n_i$ analysis frames rather than
binary successes or failures, logistic regression assumes that each $p_i$
represents the observed proportion of successes among $n_i$
conditionally-independent binary observations, or more formally, that
the $p_i$ are distributed binomially:
\begin{multline}
  \label{eq:binomial}
  f_{P \mid N, \vec{X}}(p_i \mid n_i, \vec{x}_i; \vec{\betaup}) = \\
  \binom{n_i}{p_i n_i} 
  \pi(\vec{x}_i; \vec{\betaup})^{p_i n_i} 
  \left[1 -\pi(\vec{x}_i; \vec{\betaup})\right]^{(1 - p_i) n_i}
  \ .
\end{multline}
The expected value for each $p_i$ is naturally $\pi(\vec{x}_i;
\vec{\betaup})$, the overall relative frequency of success given
$\vec{x}_i$:
\begin{equation}
  \label{eq:E}
  \mathbf{E}\left[P \mid N, \vec{X}\right] = 
  \pi(\vec{x}; \vec{\betaup})\ .
\end{equation}

Logistic regression models are typically fit by the maximum-likelihood
technique, i.e., one is seeking a vector $\vec{\hat{\betaup}}$ to
maximise the log-likelihood given the data:
\begin{multline}
  \label{eq:loglik}
  \ell_{P \mid N, \vec{X}}
  (\vec{\betaup}; \vec{p}, \vec{n}, \vec{X}) =
  \sum_i \left\{
    \log \binom{n_i}{p_i n_i} + \right. \\ \left.\phantom{\binom{x}{x}}
    p_i n_i \log \pi(\vec{x}_i; \vec{\betaup}) + 
    (1 - p_i) n_i \log \left[1 -\pi(\vec{x}_i; 
      \vec{\betaup})\right]\right\}\ .
\end{multline}
One thus solves the system of likelihood equations for
$\vec{\betaup}$, whereby the gradient of Equation~\ref{eq:loglik} is
set to zero:
\begin{equation}
  \label{eq:gradient}
  \nabla_{\vec{\betaup}}
  \ell_{P \mid N, \vec{X}} 
  (\vec{\betaup}; p_i, n_i, \vec{x}_i) =
  \sum_i 
  \left[p_i - \pi(\vec{x}_i; \vec{\betaup}) \right]
  n_i \vec{x}_i = \vec{0}
\end{equation}
and so
\begin{equation}
  \label{eq:betahat}
  \sum_i p_i n_i \vec{x}_i = 
  \sum_i \pi(\vec{x}_i; \vec{\betaup}) n_i \vec{x}_i\ .
\end{equation}
In the case of \acro{MIREX} \acro{ACE} evaluation, $\vec{x}_i$ is simply an
indicator vector that partitions the data by algorithm, and thus
$\vec{\hat{\betaup}}$ is the parameter vector for which
$\pi(\vec{x}_i; \vec{\betaup})$ equals the weighted mean of the $p_i$
for the corresponding algorithm.

\subsection{Quasi-Binomial Models}

Under a strict logit model, the variance of each $p_i$ is inversely
proportional to $n_i$:
\begin{equation}
  \label{eq:var}
  \var\left[P \mid N, \vec{X}\right] = 
  \left(\frac{1}{n}\right)
  \pi(\vec{x}; \vec{\betaup}) 
  \left[1 - \pi(\vec{x}; \vec{\betaup})\right]
  \ .
\end{equation}
Equation~\ref{eq:var} only holds, however, if the estimates of chord labels
for each audio frame are independent. For \acro{ACE}, this is
unrealistic: only the most naÃ¯ve algorithms treat every frame
independently; some kind of time-dependence structure is standard,
most frequently a hidden Markov model or some close derivative
thereof. Hence one should expect that the variance of \acro{WCSR}
estimates would be rather larger than the basic logit model would
suggest. 

This type of problem is extremely common across disciplines,
so much so that is has been given a name, \emph{over-dispersion}, and
some authors go so far as to state that \enquote{unless there are good
  external reasons for relying on the binomial assumption [of
  independence], it seems wise to be cautious and to assume that
  over-dispersion is present to some extent unless and until it is
  shown to be absent} \cite[p.~125]{McCullagh1989}.
One standard approach to handling over-dispersion is to use a
so-called \emph{quasi-likelihood}
\cite[\S\,4.7]{Agresti2007}. In case of logistic regression, this
typically entails a modification to the assumption on the distribution
of the $p_i$ that includes an additional \emph{dispersion parameter}
$\phi$:
\begin{equation}
  \label{eq:quasi-E}
  \mathbf{E}\left[P \mid N, \vec{X}\right] = 
  \pi(\vec{x}; \vec{\betaup})
\end{equation}
as before, but
\begin{equation}
  \label{eq:quasi-var}
  \var\left[P \mid N, \vec{X}\right] \triangleq 
  \left(\frac{\phi}{n}\right)
  \pi(\vec{x}; \vec{\betaup}) 
  \left[1 - \pi(\vec{x}; \vec{\betaup})\right]
  \ .
\end{equation}
These models are known as quasi-likelihood models because one loses a
closed-form solution for the actual probability distribution $f_{P
  \mid N, \vec{X}}$; one knows only that the $p_i$ behave something
like binomially-distributed variables, with identical means but
proportionally more variance. The parameter estimates
$\vec{\hat{\betaup}}$ and predictions $\pi(\cdot;
\vec{\hat{\betaup}})$ for a quasi-binomial model are the same as
ordinary logistic regression, but the estimated variance-covariance
matrices are scaled by the estimated dispersion parameter $\hat{\phi}$
(and likewise the standard errors are scaled by its square root). The
dispersion parameter is estimated so that the theoretical variance
matches the empirical variance in the data, and because of the form of
Equation~\ref{eq:quasi-var}, it renders any scaling considerations for
the $n_i$ moot.

\subsection{Generalised Estimating Equations (GEEs)}

The quasi-binomial model achieves most of what one would be looking
for when evaluating \acro{ACE} for \acro{MIREX}: it handles
proportions naturally, is consistent with the weighted averaging used
to compute \acro{WCSR}, adjusts for over-dispersion in a way that also
eliminates any worries about scaling, and above all, it seems to fit
the data well. Nonetheless, the quasi-binomial model is slightly
over-conservative for evaluating \acro{ACE}. As discussed earlier,
quasi-binomial models are necessary to account for over-dispersion,
and one important source of over-dispersion in these data is the lack
of independence of chord estimates from most algorithms within the
same song. \acro{MIREX} exhibits another important violation of the independence
assumption, however: all algorithms are tested on the same sets of
songs, and some songs are clearly more difficult than others; put
differently, one does not expect the algorithms to perform completely
independently of one another on the same song but rather expects a
certain correlation in performance across the set of songs. By taking
that correlation into account, one can improve the precision of
estimates, particularly the precision of pair-wise comparisons
\cite[\S\,10.1]{Agresti2007}.

A relatively straightforward variant of quasi-likelihood known as
\emph{generalised estimating equations} (\acro{GEE}s) incorporates this type
of correlation \cite[ch.~11]{Agresti2007}. With the \acro{GEE} approach, rather than
predicting each $p_i$ individually, one predicts complete vectors of
proportions $\vec{p}_i$ for each relevant group, much as Friedman's
test seeks to estimate ranks within each group. For \acro{ACE},
the groups are songs, and thus one considers the
observations to be vectors $\vec{p}_i$, one for each song, where
$p_{ij}$ represents the \acro{CSR} or segmentation score for algorithm $j$
on song $i$. Analogous to the case of ordinary quasi-binomial or
logistic regression,
\begin{equation}
  \label{eq:gee-E}
  \mathbf{E}\left[P_j \mid N, \vec{X}_j\right] =
  \pi(\vec{x}_j; \vec{\betaup})\ .   
\end{equation}
Likewise, analogous to the quasi-binomial variance estimate,
\begin{equation}
  \label{eq:gee-var}
  \var\left[P_j \mid N, \vec{X}_j\right] \triangleq 
  \left(\frac{\phi}{n}\right)
  \pi(\vec{x}_j; \vec{\betaup}) 
  \left[1 - \pi(\vec{x}_j); \vec{\betaup})\right]
  \ .
\end{equation}
Because the \acro{GEE} approach is concerned with vector-valued estimates
rather than point estimates, it also involves estimating a full
variance-covariance matrix. In addition to $\vec{\betaup}$ and $\phi$,
the approach requires a further vector of parameters $\vec{\alphaup}$
and an \emph{a priori} assumption on correlation structure of the
$P_j$ in the form of a function $R(\vec{\alphaup})$ that yields a
correlation matrix. (One might, for example, assume that that the
$P_j$ are \emph{exchangeable}, i.e., that every pair shares a common
correlation coefficient.) Then if $B$ is a diagonal matrix such that
$B_{jj} \triangleq \var\left[P_j \mid N, \vec{X}_j\right]$,
\begin{equation}
  \label{eq:gee-cov}
  \cov\left[P \mid N, \vec{X}\right] \triangleq
  B^{\sfrac{1}{2}} R(\vec{\alphaup}) B^{\sfrac{1}{2}}\ .
\end{equation}
If all of the $P_j$ are uncorrelated with each other, then this
formula reduces to the basic quasi-binomial model, which assumes a
diagonal covariance matrix. The final step of \acro{GEE} estimation
adjusts Equation~\ref{eq:gee-cov} according to the actual correlations
observed in the data, and as such, \acro{GEE} is quite robust in
practice even when the \emph{a priori} assumptions about the
correlation structure are incorrect \cite[\S\,11.4.2]{Agresti2007}.

\subsection{Current Evaluation Battery for ACE}

With 12 algorithms, 5 vocabularies plus a segmentation measure, and
161 songs, the complete evaluation data \emph{Billboard} 2013 test set
includes 11\,592 observations.

<<read-results>>=
ReadResults <- function (analysis) {
  #' MIREX results for a particular chord vocabulary or segmentation
  #' 
  #' Assumes that the current directory is that of a particular database in
  #' Johan's distribution format.
  #'
  #' @param analysis chord vocabulary or segmentation to load
  #'
  is.segmentation <- analysis == "Segmentation"
  col.names <- c()
  col.classes <- c()
  if (is.segmentation) {
    setwd("resultsSegmentation")
    col.names <- c("song", "", "under.segmentation", "over.segmentation")
    col.classes <- c("factor", "NULL", "numeric", "numeric")
  } else {
    setwd(paste0("resultsMirex", analysis))
    col.names <- c("song", 
                   "performance", 
                   "duration",
                   rep("", 6))
    col.classes <- c("factor", "numeric", "numeric", rep("NULL", 6))
  }
  results <- data.frame()
  for (algo in ALGOS) {
    algo.results <- read.table(paste0(algo, ".csv"),
                               skip = 2,
                               header = FALSE,
                               sep = ",",
                               dec = ".",
                               col.names = col.names, 
                               colClasses = col.classes)
    if (is.segmentation) {
      ## Replace over- and under-segmentation with the 
      ## harmonic mean of their arithmetic inverses. 
      algo.results$performance <-
        1 / (0.5 * (1/(1-algo.results$under.segmentation) + 
                    1/(1-algo.results$over.segmentation)))
      algo.results$over.segmentation <- c()
      algo.results$under.segmentation <- c()
    } else {
      ## Rescale RCO results back to [0, 1].
      algo.results$performance <- algo.results$performance/100
    }
    results <- rbind(results, 
                     cbind(algorithm = rep(algo, dim(algo.results)[1]),
                           algo.results))
  }
  setwd("..")
  if (is.segmentation) results$duration <- ReadResults("Root")$duration
  results <-
    within(results, {
      performance.rank <- rep(NA, length(performance))
      for (group in levels(song))
        performance.rank[song==group] <- rank(performance[song==group])
    })
  return(results)
}

ReadAllResults <- function (data.set) {
  #' Complete battery of MIREX results
  #' 
  #' Assumes that the current directory contains directories of results
  #' for each database.
  #' 
  #' @param data.set the data set for which to load evaluations
  #'
  setwd(data.set)
  results <-
    within(
      Reduce(rbind,
             lapply(ANALYSES, 
                    function (analysis) {
                      result <- ReadResults(analysis)
                      result$analysis <- rep(analysis, dim(result)[1])
                      return(result)
                    }),
             data.frame()),
      {
        analysis <- factor(analysis, levels=ANALYSES)
        vocabulary <- analysis
        levels(vocabulary) <- c("Root", "MajMin", "MajMin", "Sevenths", "Sevenths", NA)
        bass <- analysis == "MajMinBass" | analysis == "SeventhsBass"
        bass[analysis == "Segmentation"] <- NA
        ## Order the algorithm factor for easier comparisons.
        algorithm <- factor(algorithm, levels=ALGOS)
        ## Set the contrasts such that they represent differences 
        ## from the grand average.
        contrasts(song) <- contr.Sum(levels(song))
        contrasts(algorithm) <- contr.Sum(levels(algorithm))
      })
  setwd("..")
  return(results[order(results$song),]) # group sorting for geeglm()
}

billboard.2013 <- ReadAllResults("BillboardTest2013")
@ 


<<cld-summaries, dependson="read-results">>=
friedman.summary <-
  with(droplevels(billboard.2013[billboard.2013$analysis=="SeventhsBass",]),
       summary(glht(aov(performance.rank ~ song + algorithm),
                    mcp(algorithm = "Tukey")),
               test = adjusted("fdr")))
logistic.summary <-
  with(droplevels(billboard.2013[billboard.2013$analysis=="SeventhsBass",]),
       summary(glht(geeglm(formula = performance ~ algorithm,
                           family  = binomial(link="logit"),
                           corstr  = "exchangeable",
                           weights = duration,
                           id      = song),
                    mcp(algorithm = "Tukey")),
               test = adjusted("fdr")))
@ 

<<cld-plots, dependson=c("cld-summaries"), fig.show="hold", fig.width=7, fig.height=5, fig.out="\\linewidth", fig.cap=paste0("Boxplots and compact letter displays for the \\acro{MIREX} \\acro{ACE} 2013 results on the \\emph{Billboard} 2013 test set with vocabulary V (seventh chords and inversions). $N =$ 161 songs per algorithm. Given the respective statistical models, there are insufficient data to distinguish among algorithms sharing a letter, correcting for multiple comparisons with the Benjamini--Hochberg procedure to hold the false discovery rate at $\\alpha =$ .005 \\cite{Benjamini-Hochberg-1995}. Although Friedman's \\acro{ANOVA} detects slightly more significant pairwise differences than logistic regression (", sum(friedman.summary$test$pvalues < .005), " vs.\ ", sum(logistic.summary$test$pvalues < .005), "), it operates on a different scale than \\acro{CSR} and misorders algorithms relative to \\acro{WCSR}."), fig.subcap=c("Friedman's \\acro{ANOVA}", "Logistic Regression")>>=
## Compact letter displays require an extra top margin.
par(mar = par("mar") + c(0, 0, 5, 0))
## Trick plot.cld into using the response for the Friedman ANOVA.
friedman.cld <- cld(friedman.summary, level=.005)
friedman.cld$covar <- FALSE
plot(friedman.cld,
     xlab = "Algorithm",
     ylab = "Rank per Song (1 low; 12 high)",
     las  = 2)
plot(cld(logistic.summary, level=.005),
     xlab = "Algorithm",
     ylab = "Weighted Chord-Symbol Recall",
     las  = 2)
@ 

Isophonics and two Billboard sets. Limit ourselves to Billboard
2013. Sampling procedure and sample size.

Research design in general: i.e., all algorithms on all
songs. Measures and covariates: WCSR and segmentation; different vocabularies.

\section{Results}

\subsection{Tukey tests}

Vocabulary or vocabularies and segmentation.

\subsection{Effect of vocabulary}

<<vocabulary, dependson="read-results">>=
friedman.vocab.anova <-
  with(droplevels(billboard.2013[billboard.2013$analysis != "Segmentation",]),
       Anova(aov(performance.rank ~ song + algorithm * vocabulary * bass)))
logistic.vocab.anova <-
  with(droplevels(billboard.2013[billboard.2013$analysis != "Segmentation",]),
       anova(geeglm(performance ~ algorithm * (vocabulary + bass), 
                    family = binomial(link="logit"),
                    corstr = "exchangeable",
                    weights = duration,
                    id = song),
             geeglm(performance ~ algorithm * analysis, 
                    family = binomial(link="logit"),
                    corstr = "exchangeable",
                    weights = duration,
                    id = song)))
@ 

Under Friedman's \acro{ANOVA}, there is a significant Algorithm
$\times$ Complexity interaction, $F($\Sexpr{friedman.vocab.anova[5,2]},
\Sexpr{sum(friedman.vocab.anova[7:9,2])}$) =$
\Sexpr{round(friedman.vocab.anova[5,3], 2)},
\Sexpr{FormatP(friedman.vocab.anova[5,4])}, but inversions are
significant neither as a main effect nor as part of any
interaction. The logistic regression model, in contrast, identifies a
significant three-way Algorithm $\times$ Complexity $\times$
Inversions interaction, $\chi^2($\Sexpr{logistic.vocab.anova[1,1]}$)
=$ \Sexpr{round(logistic.vocab.anova[1,2], 2)}, \Sexpr{FormatP(logistic.vocab.anova[1,3])}.

\subsection{Correlation matrices}

\begin{table*}
  \small
  \centering  \begin{tabular}{clcD{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}D{.}{.}{1.2}}
    \toprule 
    \multicolumn{3}{c}{Algorithm} &
    \colhead{\acro{KO2}} & \colhead{\acro{NMSD2}} &
    \colhead{\acro{CB4}} & \colhead{\acro{NMSD1}} &
    \colhead{\acro{CB3}} & \colhead{\acro{KO1}} &
    \colhead{\acro{PP4}} & \colhead{\acro{PP3}} &
    \colhead{\acro{CF2}} & \colhead{\acro{NG1}} &
    \colhead{\acro{NG2}} \\
    \midrule
    & \acro{NMSD2} &&     .25^\ast\\
    & \acro{CB4}   &&     .41^\ast &  .39^\ast\\
    & \acro{NMSD1} &&     .30^\ast &  .60^\ast &  .53^\ast\\
    & \acro{CB3} &&       .34^\ast &  .10      &  .76^\ast &  .42^\ast\\
    & \acro{KO1} &&      -.04      & -.42^\ast & -.51^\ast & -.51^\ast & -.29^\ast\\
    & \acro{PP4} &&      -.22      &  .08      & -.16      &  .06      & -.07      & -.05\\
    & \acro{PP3} &&      -.49^\ast & -.46^\ast & -.61^\ast & -.53^\ast & -.37^\ast &  .68^\ast &  .22\\
    & \acro{CF2} &&       .09      &  .19      &  .24^\ast &  .42^\ast &  .17      & -.49^\ast &  .06      & -.51^\ast\\
    & \acro{NG1} &&      -.54^\ast & -.42^\ast & -.60^\ast & -.56^\ast & -.41^\ast &  .68^\ast &  .04      &  .85^\ast & -.47^\ast\\
    & \acro{NG2} &&       .09      &  .17      &  .17      &  .16      & -.03      & -.50^\ast & -.09      & -.54^\ast &  .50^\ast & -.40^\ast\\
    & \acro{SB8} &&      -.32^\ast & -.44^\ast & -.44^\ast & -.52^\ast & -.46^\ast &  .00      & -.32^\ast &  .08      & -.33^\ast &  .08      & -.16\\
    \bottomrule
  \end{tabular}
  \caption{Pearson's correlation matrix on the coefficients of the
    logistic regression fit \acro{(WCSR)} for the \emph{Billboard}
    2013 test set with vocabulary V. $N =$ 161 songs per cell. Starred
    correlations are significant at $\alpha =$ .005, controlling for false
    discoveries with the Benjamini--Hochberg procedure
    \cite{Benjamini-Hochberg-1995}. A set of algorithms stands out for
    being negatively correlated with the top performers (viz.,
    \acro{KO1}, \acro{PP3}, \acro{NG1}, and \acro{SB8}); in general,
    algorithms in this set did not attempt to recognise seventh chords.}
  \label{tab:cor}
\end{table*}

In general, algorithms that did not measure sevenths did not do
well. Difference in vocabulary accounts for the separation between NG1
and NG2, PP3 and PP4. The exception is KO1. Looking in detail at the
results, training helped immensely. Restricting the results to 7th
chords, KO1 only performs at 11 percent while KO2 achieves 46
percent. This may be due to the data available for training. Most
algorithms are trained on the Isophonics set, which has only 15
percent seventh chords, but the \emph{Billboard} set has about 29
percent seventh chords, nearly twice as many.

\subsection{Outliers}

<<outliers, dependson="read-results">>=
## Chauvenet's criterion for this sample size.
chauvenet <- qnorm(1/(2*dim(billboard.2013)[1]), lower.tail=FALSE)
friedman.outliers <- with(billboard.2013, {
  friedman.fit <- glm(performance.rank ~ song + algorithm * analysis)
  billboard.2013[abs(resid(friedman.fit)) > chauvenet * sd(resid(friedman.fit)),]
})
logistic.outliers <- with(billboard.2013, {
  logistic.fit <- geeglm(formula = performance ~ algorithm * analysis,
                         family = binomial(link="logit"),
                         corstr = "exchangeable",
                         weights = duration, id = song)
  billboard.2013[abs(resid(logistic.fit)) > chauvenet * sd(resid(logistic.fit)),]
})
@ 

Chauvenet's criterion for outliers in a sample of this size is to lie
more than \Sexpr{round(chauvenet, 2)} standard deviations from the
mean. No data point meets this criterion under Friedman's
\acro{ANOVA}. The most extreme residuals occur for algorithm
\acro{SB8}, on songs that were so difficult for most algorithms that
the essentially random approach of \acro{SB8}, due to its software
bug, did better. Under the logistic regression model, Chauvenet's
criterion identifies \Sexpr{dim(logistic.outliers)[1]} extreme data
points. These are primarily for songs that are tuned quarter-tone off
from standard tuning (A4 = 440\,Hz). The ground truth necessarily is
\enquote{rounded off} to standard tuning in one direction or the
other, but in cases where an otherwise high-performing algorithm
happened to round off in the other direction, the performance is
extremely and unexpectedly low.

\section{Discussion}

\subsection{Utility of new vocabularies}

\subsection{Comparison with Friedman test}

Comparative figure.

\subsection{Correlation matrices}

<<correlations, dependson="read-results", fig.width=7, fig.height=4.5, fig.out="\\linewidth", fig.cap="Hierarchical clustering of algorithms based on \\acro{WCSR} for for the \\emph{Billboard} 2013 test set with vocabulary V, Pearson's distance, and complete linkage. The group of algorithms that is negatively correlated with the top performers appears at the left. \\acro{PP4} stands out as the most idiosyncratic performer.">>=
with(droplevels(billboard.2013[billboard.2013$analysis=="SeventhsBass",]),
     { ## It may not be necessary to change the contrast type, but it is safer.
       .contrasts. <- contr.Sum(levels(algorithm))
       contrasts(algorithm) <- .contrasts.
       .vcov. <- vcov(geeglm(formula = performance ~ algorithm,
                             family = binomial(link="logit"),
                             corstr = "exchangeable",
                             weights = duration, id = song))
       logistic.vcov <- .contrasts. %*% .vcov.[2:12,2:12] %*% t(.contrasts.)
       logistic.cor <- cov2cor(logistic.vcov)
       logistic.dist <- as.dist((1 - logistic.cor)/2) 
       ## Confirm that the Friedman variance-covariance matrix is degenerate.
       .vcov. <- vcov(glm(performance.rank ~ algorithm + song))
       friedman.vcov <- .contrasts. %*% .vcov.[2:12,2:12] %*% t(.contrasts.)
       friedman.cor <- cov2cor(friedman.vcov)
       friedman.dist <- as.dist((1 - friedman.cor)/2)
       par(mar=par("mar")-c(4,0,0,0))
       plot(hclust(logistic.dist, method="complete"),
            main="", sub="",
            ylab="Pearson's Distance", xlab="")
     })
@ 

\subsection{Outliers}

The most extreme residuals are all for songs where \acro{SB8} did
unusually well.

Tuning is a known problem.

\subsection{Interpretation and generalisability}

\subsection{Comparison with Pauwels \& Peeters}

\section{Implications for Future Work}

\bibliography{burgoyne2014comparative}

\end{document}
